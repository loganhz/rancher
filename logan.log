./.dapper ci
Sending build context to Docker daemon 164.1 MB
Step 1/29 : FROM ubuntu:18.04
 ---> 775349758637
Step 2/29 : ARG DAPPER_HOST_ARCH
 ---> Using cache
 ---> 4834954ec08a
Step 3/29 : ENV HOST_ARCH ${DAPPER_HOST_ARCH} ARCH ${DAPPER_HOST_ARCH}
 ---> Using cache
 ---> e2f110bddcd5
Step 4/29 : ENV CATTLE_HELM_VERSION v2.14.3-rancher1
 ---> Using cache
 ---> 32fb643240de
Step 5/29 : ENV CATTLE_K3S_VERSION v0.8.0
 ---> Using cache
 ---> dd50333e6617
Step 6/29 : ENV CATTLE_ETCD_VERSION v3.3.14
 ---> Using cache
 ---> e62bf44b780f
Step 7/29 : ENV GO111MODULE off
 ---> Using cache
 ---> 97769773b4b5
Step 8/29 : RUN apt-get update &&     apt-get install -y gcc ca-certificates git wget curl vim less file xz-utils unzip &&     rm -f /bin/sh && ln -s /bin/bash /bin/sh
 ---> Using cache
 ---> 4a83d1ec9d94
Step 9/29 : RUN curl -sLf https://github.com/rancher/machine-package/releases/download/v0.15.0-rancher5-3/docker-machine-${ARCH}.tar.gz | tar xvzf - -C /usr/bin
 ---> Using cache
 ---> 132c4deabb41
Step 10/29 : ENV GOLANG_ARCH_amd64 amd64 GOLANG_ARCH_arm armv6l GOLANG_ARCH_arm64 arm64 GOLANG_ARCH GOLANG_ARCH_${ARCH} GOPATH /go PATH /go/bin:/usr/local/go/bin:${PATH} SHELL /bin/bash
 ---> Using cache
 ---> 26c9d62f0362
Step 11/29 : RUN wget -O - https://storage.googleapis.com/golang/go1.13.4.linux-${!GOLANG_ARCH}.tar.gz | tar -xzf - -C /usr/local
 ---> Using cache
 ---> 96df2d21ff6f
Step 12/29 : RUN if [ "${ARCH}" == "amd64" ]; then     curl -sL https://install.goreleaser.com/github.com/golangci/golangci-lint.sh | sh -s v1.20.0;     fi
 ---> Using cache
 ---> f0d9e04d680e
Step 13/29 : ENV DOCKER_URL_amd64 https://get.docker.com/builds/Linux/x86_64/docker-1.10.3 DOCKER_URL_arm https://github.com/rancher/docker/releases/download/v1.10.3-ros1/docker-1.10.3_arm DOCKER_URL_arm64 https://github.com/rancher/docker/releases/download/v1.10.3-ros1/docker-1.10.3_arm64 DOCKER_URL DOCKER_URL_${ARCH}
 ---> Using cache
 ---> 9b8f268eba35
Step 14/29 : ENV HELM_URL_amd64 https://github.com/rancher/helm/releases/download/${CATTLE_HELM_VERSION}/rancher-helm HELM_URL_arm64 https://github.com/rancher/helm/releases/download/${CATTLE_HELM_VERSION}/rancher-helm-arm64 HELM_URL HELM_URL_${ARCH} TILLER_URL_amd64 https://github.com/rancher/helm/releases/download/${CATTLE_HELM_VERSION}/rancher-tiller TILLER_URL_arm64 https://github.com/rancher/helm/releases/download/${CATTLE_HELM_VERSION}/rancher-tiller-arm64 TILLER_URL TILLER_URL_${ARCH} K3S_URL_amd64 https://github.com/rancher/k3s/releases/download/${CATTLE_K3S_VERSION}/k3s K3S_URL_arm64 https://github.com/rancher/k3s/releases/download/${CATTLE_K3S_VERSION}/k3s-arm64 K3S_URL K3S_URL_${ARCH} ETCD_URL_amd64 https://github.com/etcd-io/etcd/releases/download/${CATTLE_ETCD_VERSION}/etcd-${CATTLE_ETCD_VERSION}-linux-amd64.tar.gz ETCD_URL_arm64 https://github.com/etcd-io/etcd/releases/download/${CATTLE_ETCD_VERSION}/etcd-${CATTLE_ETCD_VERSION}-linux-arm64.tar.gz ETCD_URL ETCD_URL_${ARCH}
 ---> Using cache
 ---> fdffc892e6a4
Step 15/29 : RUN curl -sLf ${!HELM_URL} > /usr/bin/rancher-helm &&     curl -sLf ${!TILLER_URL} > /usr/bin/rancher-tiller &&     curl -sLf ${!K3S_URL} > /usr/bin/k3s &&     curl -sfL ${!ETCD_URL} | tar xvzf - --strip-components=1 -C /usr/bin/ etcd-${CATTLE_ETCD_VERSION}-linux-${ARCH}/etcd &&     chmod +x /usr/bin/rancher-helm /usr/bin/rancher-tiller /usr/bin/k3s &&     ln -s /usr/bin/rancher-helm /usr/bin/helm &&     ln -s /usr/bin/rancher-tiller /usr/bin/tiller &&     rancher-helm init -c &&     rancher-helm plugin install https://github.com/rancher/helm-unittest &&     mkdir -p /go/src/github.com/rancher/rancher/.kube &&     ln -s /etc/rancher/k3s/k3s.yaml /go/src/github.com/rancher/rancher/.kube/k3s.yaml
 ---> Using cache
 ---> 6c85529ea250
Step 16/29 : RUN wget -O - ${!DOCKER_URL} > /usr/bin/docker && chmod +x /usr/bin/docker
 ---> Using cache
 ---> 24b19fb5fcec
Step 17/29 : ENV KUBECTL_URL https://storage.googleapis.com/kubernetes-release/release/v1.11.0/bin/linux/${ARCH}/kubectl
 ---> Using cache
 ---> 5daab3b1df90
Step 18/29 : RUN wget -O - ${KUBECTL_URL} > /usr/bin/kubectl && chmod +x /usr/bin/kubectl
 ---> Using cache
 ---> 5469dc892b73
Step 19/29 : RUN apt-get update &&     apt-get install -y tox python3.7 python3-dev python3.7-dev libffi-dev libssl-dev
 ---> Using cache
 ---> 50c08309aeac
Step 20/29 : ENV HELM_HOME /root/.helm
 ---> Using cache
 ---> f47d12079a6b
Step 21/29 : ENV DAPPER_ENV REPO TAG DRONE_TAG SYSTEM_CHART_DEFAULT_BRANCH
 ---> Using cache
 ---> ef0ff75483a5
Step 22/29 : ENV DAPPER_SOURCE /go/src/github.com/rancher/rancher/
 ---> Using cache
 ---> 8e667fcd4a71
Step 23/29 : ENV DAPPER_OUTPUT ./bin ./dist
 ---> Using cache
 ---> 15e8481e5f9c
Step 24/29 : ENV DAPPER_DOCKER_SOCKET true
 ---> Using cache
 ---> f85147b8b96e
Step 25/29 : ENV TRASH_CACHE ${DAPPER_SOURCE}/.trash-cache
 ---> Using cache
 ---> 142f6aa256ec
Step 26/29 : ENV HOME ${DAPPER_SOURCE}
 ---> Using cache
 ---> 6b3c106c85d0
Step 27/29 : WORKDIR ${DAPPER_SOURCE}
 ---> Using cache
 ---> 7ff2102982c4
Step 28/29 : ENTRYPOINT ./scripts/entry
 ---> Using cache
 ---> e196e1ca1427
Step 29/29 : CMD ci
 ---> Using cache
 ---> b5d9b3405fd3
Successfully built b5d9b3405fd3
Sending build context to Docker daemon 164.1 MB
Step 1/2 : FROM rancher:update-node-labels
 ---> b5d9b3405fd3
Step 2/2 : COPY . /go/src/github.com/rancher/rancher/
 ---> e610b6999688
Removing intermediate container 7979a14a3878
Successfully built e610b6999688
Running: build-server
ARCH: amd64
CHART_REPO: dev
CHART_VERSION: 0.0.0-dirty.448d60d2e
VERSION: 448d60d2e-dirty
Running: build-agent
ARCH: amd64
CHART_REPO: dev
CHART_VERSION: 0.0.0-dirty.448d60d2e
VERSION: 448d60d2e-dirty
Starting rancher server
pongRunning tests
GLOB sdist-make: /go/src/github.com/rancher/rancher/tests/integration/setup.py
flake8 create: /go/src/github.com/rancher/rancher/tests/integration/.tox/flake8
flake8 installdeps: -rrequirements.txt
flake8 inst: /go/src/github.com/rancher/rancher/tests/integration/.tox/dist/IntegrationTests-0.1.zip
flake8 installed: apipkg==1.5,argcomplete==1.10.3,asn1crypto==1.2.0,atomicwrites==1.3.0,attrs==19.3.0,cachetools==3.1.1,certifi==2019.11.28,cffi==1.13.2,chardet==3.0.4,Click==7.0,client-python==0.1.0,cryptography==2.6.1,entrypoints==0.3,execnet==1.7.1,flake8==3.7.7,Flask==1.0.2,google-auth==1.7.1,idna==2.8,importlib-metadata==1.2.0,IntegrationTests==0.1,itsdangerous==1.1.0,Jinja2==2.10.3,kubernetes==9.0.0,MarkupSafe==1.1.1,mccabe==0.6.1,more-itertools==8.0.0,netaddr==0.7.19,oauthlib==3.1.0,pkg-resources==0.0.0,pluggy==0.13.1,py==1.8.0,pyasn1==0.4.8,pyasn1-modules==0.2.7,pycodestyle==2.5.0,pycparser==2.19,pyflakes==2.1.1,PyJWT==1.7.1,pytest==4.4.1,pytest-forked==1.1.3,pytest-repeat==0.8.0,pytest-xdist==1.28.0,python-dateutil==2.8.1,PyYAML==5.1,requests==2.21.0,requests-oauthlib==1.3.0,rsa==4.0,six==1.13.0,urllib3==1.24.3,websocket-client==0.56.0,Werkzeug==0.16.0,zipp==0.6.0
flake8 runtests: PYTHONHASHSEED='3621001079'
flake8 runtests: commands[0] | flake8 suite
py37 create: /go/src/github.com/rancher/rancher/tests/integration/.tox/py37
py37 installdeps: -rrequirements.txt
py37 inst: /go/src/github.com/rancher/rancher/tests/integration/.tox/dist/IntegrationTests-0.1.zip
py37 installed: apipkg==1.5,argcomplete==1.10.3,asn1crypto==1.2.0,atomicwrites==1.3.0,attrs==19.3.0,cachetools==3.1.1,certifi==2019.11.28,cffi==1.13.2,chardet==3.0.4,Click==7.0,client-python==0.1.0,cryptography==2.6.1,entrypoints==0.3,execnet==1.7.1,flake8==3.7.7,Flask==1.0.2,google-auth==1.7.1,idna==2.8,importlib-metadata==1.2.0,IntegrationTests==0.1,itsdangerous==1.1.0,Jinja2==2.10.3,kubernetes==9.0.0,MarkupSafe==1.1.1,mccabe==0.6.1,more-itertools==8.0.0,netaddr==0.7.19,oauthlib==3.1.0,pkg-resources==0.0.0,pluggy==0.13.1,py==1.8.0,pyasn1==0.4.8,pyasn1-modules==0.2.7,pycodestyle==2.5.0,pycparser==2.19,pyflakes==2.1.1,PyJWT==1.7.1,pytest==4.4.1,pytest-forked==1.1.3,pytest-repeat==0.8.0,pytest-xdist==1.28.0,python-dateutil==2.8.1,PyYAML==5.1,requests==2.21.0,requests-oauthlib==1.3.0,rsa==4.0,six==1.13.0,urllib3==1.24.3,websocket-client==0.56.0,Werkzeug==0.16.0,zipp==0.6.0
py37 runtests: PYTHONHASHSEED='3621001079'
py37 runtests: commands[0] | pytest --durations=20 -rfE -v -m not nonparallel -n 2
============================= test session starts ==============================
platform linux -- Python 3.7.5, pytest-4.4.1, py-1.8.0, pluggy-0.13.1 -- /go/src/github.com/rancher/rancher/tests/integration/.tox/py37/bin/python3.7
cachedir: .pytest_cache
rootdir: /go/src/github.com/rancher/rancher/tests/integration
plugins: repeat-0.8.0, xdist-1.28.0, forked-1.1.3
gw0 I / gw1 I
[gw0] linux Python 3.7.5 cwd: /go/src/github.com/rancher/rancher/tests/integration/suite
[gw1] linux Python 3.7.5 cwd: /go/src/github.com/rancher/rancher/tests/integration/suite
[gw0] Python 3.7.5 (default, Nov  7 2019, 10:50:52)  -- [GCC 8.3.0]
[gw1] Python 3.7.5 (default, Nov  7 2019, 10:50:52)  -- [GCC 8.3.0]
gw0 [11] / gw1 [11]

scheduling tests via LoadScheduling

test_node.py::test_node_fields 
test_node.py::test_node_template_delete 
[gw0] [  9%] PASSED test_node.py::test_node_fields 
test_node.py::test_cloud_credential_delete 
[gw1] [ 18%] PASSED test_node.py::test_node_template_delete 
test_node.py::test_writing_config_to_disk 
[gw1] [ 27%] PASSED test_node.py::test_writing_config_to_disk 
[gw0] [ 36%] PASSED test_node.py::test_cloud_credential_delete 
test_node.py::test_node_driver_schema 
[gw0] [ 45%] PASSED test_node.py::test_node_driver_schema 
test_node.py::test_admin_access_to_node_template 
[gw0] [ 54%] PASSED test_node.py::test_admin_access_to_node_template 
test_node.py::test_user_access_to_other_template 
test_node.py::test_user_access_to_node_template 
[gw0] [ 63%] PASSED test_node.py::test_user_access_to_node_template 
test_node.py::test_no_node_template 
[gw1] [ 72%] PASSED test_node.py::test_user_access_to_other_template 
test_node.py::test_admin_access_user_template 
[gw0] [ 81%] PASSED test_node.py::test_no_node_template 
test_node.py::test_add_node_label 
[gw1] [ 90%] PASSED test_node.py::test_admin_access_user_template 
[gw0] [100%] FAILED test_node.py::test_add_node_label 

=================================== FAILURES ===================================
_____________________________ test_add_node_label ______________________________
[gw0] linux -- Python 3.7.5 /go/src/github.com/rancher/rancher/tests/integration/.tox/py37/bin/python3.7

admin_mc = <suite.conftest.ManagementContext object at 0x7fe2e6125750>

    def test_add_node_label(admin_mc):
        testLabel = "test-label"
        client = admin_mc.client
        nodes = client.list_node(clusterId="local")
        nodeId = nodes.data[1].id
        node = client.by_id_node(nodeId)
    
        # Make sure there is no test label and add test label
        if "labels" in node:
            node_labels = node.labels.data_dict()
        else:
            node_labels = {}
    
        assert testLabel not in node_labels
        node_labels[testLabel] = "bar"
        client.update(node, labels=node_labels)
    
        # Label should be added
        time.sleep(20)
        node = client.by_id_node(nodeId)
>       node_labels = node.labels.data_dict()

test_node.py:373: 
_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ 

self = {'baseType': 'node', 'clusterId': 'local', 'conditions': [{'status': 'True', 'type': 'Initialized'}, {'message': 'Runn...ate checks...', 'type': 'node', 'unschedulable': False, 'uuid': '3716585e-1767-11ea-9b8c-0242ac110002', 'worker': True}
k = 'labels'

    def __getattr__(self, k):
        if self._is_list() and k in LIST_METHODS:
            return getattr(self.data, k)
>       return getattr(self.__dict__, k)
E       AttributeError: 'dict' object has no attribute 'labels'

../.tox/py37/lib/python3.7/site-packages/rancher.py:89: AttributeError
=========================== short test summary info ============================
FAILED test_node.py::test_add_node_label
========================== slowest 20 test durations ===========================
20.04s call     suite/test_node.py::test_add_node_label
2.07s teardown suite/test_node.py::test_writing_config_to_disk
2.05s teardown suite/test_node.py::test_admin_access_to_node_template
1.51s call     suite/test_node.py::test_user_access_to_other_template
1.05s setup    suite/test_node.py::test_node_fields
1.00s setup    suite/test_node.py::test_node_template_delete
0.59s call     suite/test_node.py::test_cloud_credential_delete
0.58s setup    suite/test_node.py::test_no_node_template
0.56s setup    suite/test_node.py::test_user_access_to_node_template
0.54s setup    suite/test_node.py::test_admin_access_user_template
0.51s call     suite/test_node.py::test_user_access_to_node_template
0.16s call     suite/test_node.py::test_node_template_delete
0.13s call     suite/test_node.py::test_writing_config_to_disk
0.09s teardown suite/test_node.py::test_user_access_to_other_template
0.05s teardown suite/test_node.py::test_admin_access_user_template
0.03s teardown suite/test_node.py::test_no_node_template
0.03s teardown suite/test_node.py::test_user_access_to_node_template
0.02s call     suite/test_node.py::test_admin_access_to_node_template
0.02s call     suite/test_node.py::test_admin_access_user_template
0.01s teardown suite/test_node.py::test_node_template_delete
===================== 1 failed, 10 passed in 26.60 seconds =====================
ERROR: InvocationError: '/go/src/github.com/rancher/rancher/tests/integration/.tox/py37/bin/pytest --durations=20 -rfE -v -m not nonparallel -n 2'
___________________________________ summary ____________________________________
  flake8: commands succeeded
ERROR:   py37: commands failed
Stopping rancher server
Cleanup DIND
***RANCHER LOGS***
Starting rancher server
2019/12/05 13:56:47 [INFO] Rancher version 448d60d2e-dirty is starting
2019/12/05 13:56:47 [INFO] Rancher arguments {ACMEDomains:[] AddLocal:true Embedded:false KubeConfig: HTTPListenPort:8080 HTTPSListenPort:8443 K8sMode:auto Debug:false NoCACerts:false ListenConfig:<nil> AuditLogPath:/var/log/auditlog/rancher-api-audit.log AuditLogMaxage:10 AuditLogMaxsize:100 AuditLogMaxbackup:10 AuditLevel:0 Features:}
2019/12/05 13:56:47 [INFO] Listening on /tmp/log.sock
2019/12/05 13:56:47 [INFO] Running etcd --data-dir=management-state/etcd
2019-12-05 13:56:47.226009 W | pkg/flags: unrecognized environment variable ETCD_URL_arm64=https://github.com/etcd-io/etcd/releases/download/v3.3.14/etcd-v3.3.14-linux-arm64.tar.gz
2019-12-05 13:56:47.226042 W | pkg/flags: unrecognized environment variable ETCD_URL_amd64=https://github.com/etcd-io/etcd/releases/download/v3.3.14/etcd-v3.3.14-linux-amd64.tar.gz
2019-12-05 13:56:47.226050 W | pkg/flags: unrecognized environment variable ETCD_URL=ETCD_URL_amd64
2019-12-05 13:56:47.226067 I | etcdmain: etcd Version: 3.3.14
2019-12-05 13:56:47.226072 I | etcdmain: Git SHA: 5cf5d88a1
2019-12-05 13:56:47.226077 I | etcdmain: Go Version: go1.12.9
2019-12-05 13:56:47.226082 I | etcdmain: Go OS/Arch: linux/amd64
2019-12-05 13:56:47.226088 I | etcdmain: setting maximum number of CPUs to 2, total number of available CPUs is 2
2019-12-05 13:56:47.226343 I | embed: listening for peers on http://localhost:2380
2019-12-05 13:56:47.226426 I | embed: listening for client requests on localhost:2379
2019-12-05 13:56:47.229709 I | etcdserver: name = default
2019-12-05 13:56:47.229726 I | etcdserver: data dir = management-state/etcd
2019-12-05 13:56:47.229733 I | etcdserver: member dir = management-state/etcd/member
2019-12-05 13:56:47.229739 I | etcdserver: heartbeat = 100ms
2019-12-05 13:56:47.229744 I | etcdserver: election = 1000ms
2019-12-05 13:56:47.229749 I | etcdserver: snapshot count = 100000
2019-12-05 13:56:47.229758 I | etcdserver: advertise client URLs = http://localhost:2379
2019-12-05 13:56:47.229764 I | etcdserver: initial advertise peer URLs = http://localhost:2380
2019-12-05 13:56:47.229772 I | etcdserver: initial cluster = default=http://localhost:2380
2019-12-05 13:56:47.233810 I | etcdserver: starting member 8e9e05c52164694d in cluster cdf818194e3a8c32
2019-12-05 13:56:47.233833 I | raft: 8e9e05c52164694d became follower at term 0
2019-12-05 13:56:47.233843 I | raft: newRaft 8e9e05c52164694d [peers: [], term: 0, commit: 0, applied: 0, lastindex: 0, lastterm: 0]
2019-12-05 13:56:47.233870 I | raft: 8e9e05c52164694d became follower at term 1
2019-12-05 13:56:47.239089 W | auth: simple token is not cryptographically signed
2019-12-05 13:56:47.241542 I | etcdserver: starting server... [version: 3.3.14, cluster version: to_be_decided]
2019-12-05 13:56:47.242378 I | etcdserver: 8e9e05c52164694d as single-node; fast-forwarding 9 ticks (election ticks 10)
2019-12-05 13:56:47.242614 I | etcdserver/membership: added member 8e9e05c52164694d [http://localhost:2380] to cluster cdf818194e3a8c32
2019-12-05 13:56:47.534200 I | raft: 8e9e05c52164694d is starting a new election at term 1
2019-12-05 13:56:47.534249 I | raft: 8e9e05c52164694d became candidate at term 2
2019-12-05 13:56:47.534265 I | raft: 8e9e05c52164694d received MsgVoteResp from 8e9e05c52164694d at term 2
2019-12-05 13:56:47.534279 I | raft: 8e9e05c52164694d became leader at term 2
2019-12-05 13:56:47.534288 I | raft: raft.node: 8e9e05c52164694d elected leader 8e9e05c52164694d at term 2
2019-12-05 13:56:47.534422 I | etcdserver: setting up the initial cluster version to 3.3
2019-12-05 13:56:47.535288 N | etcdserver/membership: set the initial cluster version to 3.3
2019-12-05 13:56:47.535330 I | etcdserver/api: enabled capabilities for version 3.3
2019-12-05 13:56:47.535355 I | etcdserver: published {Name:default ClientURLs:[http://localhost:2379]} to cluster cdf818194e3a8c32
2019-12-05 13:56:47.535468 I | embed: ready to serve client requests
2019-12-05 13:56:47.535988 N | embed: serving insecure client requests on 127.0.0.1:2379, this is strongly discouraged!
2019/12/05 13:56:48 [INFO] Waiting for k3s to start
time="2019-12-05T13:56:48Z" level=info msg="Preparing data dir /var/lib/rancher/k3s/data/de37a675b342fcd56e57fd5707882786b0e0c840862d6ddc1e8f5c391fb424c9"
2019/12/05 13:56:49 [INFO] Waiting for k3s to start
time="2019-12-05T13:56:49.508409967Z" level=info msg="Starting k3s v0.8.0 (f867995f)"
time="2019-12-05T13:56:49.723651527Z" level=info msg="Running kube-apiserver --advertise-port=6443 --allow-privileged=true --api-audiences=unknown --authorization-mode=Node,RBAC --basic-auth-file=/var/lib/rancher/k3s/server/cred/passwd --bind-address=127.0.0.1 --cert-dir=/var/lib/rancher/k3s/server/tls/temporary-certs --client-ca-file=/var/lib/rancher/k3s/server/tls/client-ca.crt --enable-admission-plugins=NodeRestriction --etcd-servers=http://localhost:2379 --insecure-port=0 --kubelet-client-certificate=/var/lib/rancher/k3s/server/tls/client-kube-apiserver.crt --kubelet-client-key=/var/lib/rancher/k3s/server/tls/client-kube-apiserver.key --proxy-client-cert-file=/var/lib/rancher/k3s/server/tls/client-auth-proxy.crt --proxy-client-key-file=/var/lib/rancher/k3s/server/tls/client-auth-proxy.key --requestheader-allowed-names=system:auth-proxy --requestheader-client-ca-file=/var/lib/rancher/k3s/server/tls/request-header-ca.crt --requestheader-extra-headers-prefix=X-Remote-Extra- --requestheader-group-headers=X-Remote-Group --requestheader-username-headers=X-Remote-User --secure-port=6444 --service-account-issuer=k3s --service-account-key-file=/var/lib/rancher/k3s/server/tls/service.key --service-account-signing-key-file=/var/lib/rancher/k3s/server/tls/service.key --service-cluster-ip-range=10.43.0.0/16 --storage-backend=etcd3 --tls-cert-file=/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.crt --tls-private-key-file=/var/lib/rancher/k3s/server/tls/serving-kube-apiserver.key"
2019/12/05 13:56:50 [INFO] Waiting for k3s to start
E1205 13:56:50.225601    4997 prometheus.go:138] failed to register depth metric admission_quota_controller: duplicate metrics collector registration attempted
E1205 13:56:50.226439    4997 prometheus.go:150] failed to register adds metric admission_quota_controller: duplicate metrics collector registration attempted
E1205 13:56:50.226497    4997 prometheus.go:162] failed to register latency metric admission_quota_controller: duplicate metrics collector registration attempted
E1205 13:56:50.226561    4997 prometheus.go:174] failed to register work_duration metric admission_quota_controller: duplicate metrics collector registration attempted
E1205 13:56:50.226594    4997 prometheus.go:189] failed to register unfinished_work_seconds metric admission_quota_controller: duplicate metrics collector registration attempted
E1205 13:56:50.226614    4997 prometheus.go:202] failed to register longest_running_processor_microseconds metric admission_quota_controller: duplicate metrics collector registration attempted
W1205 13:56:50.360241    4997 genericapiserver.go:315] Skipping API batch/v2alpha1 because it has no resources.
W1205 13:56:50.366085    4997 genericapiserver.go:315] Skipping API node.k8s.io/v1alpha1 because it has no resources.
E1205 13:56:50.385047    4997 prometheus.go:138] failed to register depth metric admission_quota_controller: duplicate metrics collector registration attempted
E1205 13:56:50.385095    4997 prometheus.go:150] failed to register adds metric admission_quota_controller: duplicate metrics collector registration attempted
E1205 13:56:50.385359    4997 prometheus.go:162] failed to register latency metric admission_quota_controller: duplicate metrics collector registration attempted
E1205 13:56:50.385559    4997 prometheus.go:174] failed to register work_duration metric admission_quota_controller: duplicate metrics collector registration attempted
E1205 13:56:50.385590    4997 prometheus.go:189] failed to register unfinished_work_seconds metric admission_quota_controller: duplicate metrics collector registration attempted
E1205 13:56:50.385609    4997 prometheus.go:202] failed to register longest_running_processor_microseconds metric admission_quota_controller: duplicate metrics collector registration attempted
time="2019-12-05T13:56:50.408861930Z" level=info msg="Running kube-scheduler --bind-address=127.0.0.1 --kubeconfig=/var/lib/rancher/k3s/server/cred/scheduler.kubeconfig --port=10251 --secure-port=0"
time="2019-12-05T13:56:50.421371720Z" level=info msg="Running kube-controller-manager --allocate-node-cidrs=true --bind-address=127.0.0.1 --cluster-cidr=10.42.0.0/16 --cluster-signing-cert-file=/var/lib/rancher/k3s/server/tls/server-ca.crt --cluster-signing-key-file=/var/lib/rancher/k3s/server/tls/server-ca.key --kubeconfig=/var/lib/rancher/k3s/server/cred/controller.kubeconfig --port=10252 --root-ca-file=/var/lib/rancher/k3s/server/tls/server-ca.crt --secure-port=0 --service-account-private-key-file=/var/lib/rancher/k3s/server/tls/service.key --use-service-account-credentials=true"
W1205 13:56:50.426985    4997 authorization.go:47] Authorization is disabled
W1205 13:56:50.427001    4997 authentication.go:55] Authentication is disabled
E1205 13:56:50.496049    4997 reflector.go:126] k8s.io/client-go/informers/factory.go:130: Failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1205 13:56:50.496166    4997 reflector.go:126] k8s.io/client-go/informers/factory.go:130: Failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1205 13:56:50.496248    4997 reflector.go:126] k8s.io/client-go/informers/factory.go:130: Failed to list *v1beta1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1205 13:56:50.497232    4997 reflector.go:126] k8s.io/kubernetes/cmd/kube-scheduler/app/server.go:223: Failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
time="2019-12-05T13:56:50.498978189Z" level=info msg="Creating CRD listenerconfigs.k3s.cattle.io"
E1205 13:56:50.503960    4997 reflector.go:126] k8s.io/client-go/informers/factory.go:130: Failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1205 13:56:50.503999    4997 reflector.go:126] k8s.io/client-go/informers/factory.go:130: Failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1205 13:56:50.504388    4997 leaderelection.go:306] error retrieving resource lock kube-system/kube-controller-manager: endpoints "kube-controller-manager" is forbidden: User "system:kube-controller-manager" cannot get resource "endpoints" in API group "" in the namespace "kube-system"
E1205 13:56:50.505295    4997 reflector.go:126] k8s.io/client-go/informers/factory.go:130: Failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
E1205 13:56:50.505368    4997 reflector.go:126] k8s.io/client-go/informers/factory.go:130: Failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1205 13:56:50.507529    4997 reflector.go:126] k8s.io/client-go/informers/factory.go:130: Failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1205 13:56:50.507625    4997 reflector.go:126] k8s.io/client-go/informers/factory.go:130: Failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
time="2019-12-05T13:56:50.508762733Z" level=info msg="Creating CRD addons.k3s.cattle.io"
E1205 13:56:50.510075    4997 controller.go:147] Unable to perform initial Kubernetes service initialization: Service "kubernetes" is invalid: spec.clusterIP: Invalid value: "10.43.0.1": cannot allocate resources of type serviceipallocations at this time
E1205 13:56:50.510556    4997 controller.go:152] Unable to remove old endpoints from kubernetes service: StorageError: key not found, Code: 1, Key: /registry/masterleases/172.17.0.2, ResourceVersion: 0, AdditionalErrorMsg: 
time="2019-12-05T13:56:50.511659096Z" level=info msg="Creating CRD helmcharts.helm.cattle.io"
time="2019-12-05T13:56:50.522766152Z" level=info msg="Waiting for CRD listenerconfigs.k3s.cattle.io to become available"
time="2019-12-05T13:56:51.024900386Z" level=info msg="Done waiting for CRD listenerconfigs.k3s.cattle.io to become available"
time="2019-12-05T13:56:51.024931133Z" level=info msg="Waiting for CRD addons.k3s.cattle.io to become available"
2019/12/05 13:56:51 [INFO] Waiting for k3s to start
E1205 13:56:51.497177    4997 reflector.go:126] k8s.io/client-go/informers/factory.go:130: Failed to list *v1.Service: services is forbidden: User "system:kube-scheduler" cannot list resource "services" in API group "" at the cluster scope
E1205 13:56:51.501809    4997 reflector.go:126] k8s.io/client-go/informers/factory.go:130: Failed to list *v1.PersistentVolumeClaim: persistentvolumeclaims is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumeclaims" in API group "" at the cluster scope
E1205 13:56:51.508235    4997 reflector.go:126] k8s.io/client-go/informers/factory.go:130: Failed to list *v1beta1.PodDisruptionBudget: poddisruptionbudgets.policy is forbidden: User "system:kube-scheduler" cannot list resource "poddisruptionbudgets" in API group "policy" at the cluster scope
E1205 13:56:51.515486    4997 reflector.go:126] k8s.io/kubernetes/cmd/kube-scheduler/app/server.go:223: Failed to list *v1.Pod: pods is forbidden: User "system:kube-scheduler" cannot list resource "pods" in API group "" at the cluster scope
E1205 13:56:51.517425    4997 reflector.go:126] k8s.io/client-go/informers/factory.go:130: Failed to list *v1.PersistentVolume: persistentvolumes is forbidden: User "system:kube-scheduler" cannot list resource "persistentvolumes" in API group "" at the cluster scope
E1205 13:56:51.520066    4997 reflector.go:126] k8s.io/client-go/informers/factory.go:130: Failed to list *v1.StatefulSet: statefulsets.apps is forbidden: User "system:kube-scheduler" cannot list resource "statefulsets" in API group "apps" at the cluster scope
E1205 13:56:51.521479    4997 reflector.go:126] k8s.io/client-go/informers/factory.go:130: Failed to list *v1.ReplicaSet: replicasets.apps is forbidden: User "system:kube-scheduler" cannot list resource "replicasets" in API group "apps" at the cluster scope
time="2019-12-05T13:56:51.526836769Z" level=info msg="Done waiting for CRD addons.k3s.cattle.io to become available"
time="2019-12-05T13:56:51.526860464Z" level=info msg="Waiting for CRD helmcharts.helm.cattle.io to become available"
E1205 13:56:51.528699    4997 reflector.go:126] k8s.io/client-go/informers/factory.go:130: Failed to list *v1.ReplicationController: replicationcontrollers is forbidden: User "system:kube-scheduler" cannot list resource "replicationcontrollers" in API group "" at the cluster scope
E1205 13:56:51.536054    4997 reflector.go:126] k8s.io/client-go/informers/factory.go:130: Failed to list *v1.Node: nodes is forbidden: User "system:kube-scheduler" cannot list resource "nodes" in API group "" at the cluster scope
E1205 13:56:51.537853    4997 reflector.go:126] k8s.io/client-go/informers/factory.go:130: Failed to list *v1.StorageClass: storageclasses.storage.k8s.io is forbidden: User "system:kube-scheduler" cannot list resource "storageclasses" in API group "storage.k8s.io" at the cluster scope
time="2019-12-05T13:56:52.028765713Z" level=info msg="Done waiting for CRD helmcharts.helm.cattle.io to become available"
time="2019-12-05T13:56:52.037216985Z" level=info msg="Writing static file: /var/lib/rancher/k3s/server/static/charts/traefik-1.64.0.tgz"
time="2019-12-05T13:56:52.037473596Z" level=info msg="Writing manifest: /var/lib/rancher/k3s/server/manifests/rolebindings.yaml"
E1205 13:56:52.037644    4997 prometheus.go:138] failed to register depth metric k3s.cattle.io/v1, Kind=Addon: descriptor Desc{fqName: "k3s.cattle.io/v1, Kind=Addon_depth", help: "(Deprecated) Current depth of workqueue: k3s.cattle.io/v1, Kind=Addon", constLabels: {}, variableLabels: []} is invalid: "k3s.cattle.io/v1, Kind=Addon_depth" is not a valid metric name
E1205 13:56:52.037673    4997 prometheus.go:150] failed to register adds metric k3s.cattle.io/v1, Kind=Addon: descriptor Desc{fqName: "k3s.cattle.io/v1, Kind=Addon_adds", help: "(Deprecated) Total number of adds handled by workqueue: k3s.cattle.io/v1, Kind=Addon", constLabels: {}, variableLabels: []} is invalid: "k3s.cattle.io/v1, Kind=Addon_adds" is not a valid metric name
E1205 13:56:52.037755    4997 prometheus.go:162] failed to register latency metric k3s.cattle.io/v1, Kind=Addon: descriptor Desc{fqName: "k3s.cattle.io/v1, Kind=Addon_queue_latency", help: "(Deprecated) How long an item stays in workqueuek3s.cattle.io/v1, Kind=Addon before being requested.", constLabels: {}, variableLabels: []} is invalid: "k3s.cattle.io/v1, Kind=Addon_queue_latency" is not a valid metric name
E1205 13:56:52.037796    4997 prometheus.go:174] failed to register work_duration metric k3s.cattle.io/v1, Kind=Addon: descriptor Desc{fqName: "k3s.cattle.io/v1, Kind=Addon_work_duration", help: "(Deprecated) How long processing an item from workqueuek3s.cattle.io/v1, Kind=Addon takes.", constLabels: {}, variableLabels: []} is invalid: "k3s.cattle.io/v1, Kind=Addon_work_duration" is not a valid metric name
E1205 13:56:52.037824    4997 prometheus.go:189] failed to register unfinished_work_seconds metric k3s.cattle.io/v1, Kind=Addon: descriptor Desc{fqName: "k3s.cattle.io/v1, Kind=Addon_unfinished_work_seconds", help: "(Deprecated) How many seconds of work k3s.cattle.io/v1, Kind=Addon has done that is in progress and hasn't been observed by work_duration. Large values indicate stuck threads. One can deduce the number of stuck threads by observing the rate at which this increases.", constLabels: {}, variableLabels: []} is invalid: "k3s.cattle.io/v1, Kind=Addon_unfinished_work_seconds" is not a valid metric name
E1205 13:56:52.037869    4997 prometheus.go:202] failed to register longest_running_processor_microseconds metric k3s.cattle.io/v1, Kind=Addon: descriptor Desc{fqName: "k3s.cattle.io/v1, Kind=Addon_longest_running_processor_microseconds", help: "(Deprecated) How many microseconds has the longest running processor for k3s.cattle.io/v1, Kind=Addon been running.", constLabels: {}, variableLabels: []} is invalid: "k3s.cattle.io/v1, Kind=Addon_longest_running_processor_microseconds" is not a valid metric name
E1205 13:56:52.037915    4997 prometheus.go:214] failed to register retries metric k3s.cattle.io/v1, Kind=Addon: descriptor Desc{fqName: "k3s.cattle.io/v1, Kind=Addon_retries", help: "(Deprecated) Total number of retries handled by workqueue: k3s.cattle.io/v1, Kind=Addon", constLabels: {}, variableLabels: []} is invalid: "k3s.cattle.io/v1, Kind=Addon_retries" is not a valid metric name
time="2019-12-05T13:56:52.040441980Z" level=error msg="Update cert unable to convert string to cert: Unable to split cert into two parts"
time="2019-12-05T13:56:52.040490692Z" level=info msg="Listening on :6443"
E1205 13:56:52.046083    4997 prometheus.go:138] failed to register depth metric k3s.cattle.io/v1, Kind=ListenerConfig: descriptor Desc{fqName: "k3s.cattle.io/v1, Kind=ListenerConfig_depth", help: "(Deprecated) Current depth of workqueue: k3s.cattle.io/v1, Kind=ListenerConfig", constLabels: {}, variableLabels: []} is invalid: "k3s.cattle.io/v1, Kind=ListenerConfig_depth" is not a valid metric name
E1205 13:56:52.046120    4997 prometheus.go:150] failed to register adds metric k3s.cattle.io/v1, Kind=ListenerConfig: descriptor Desc{fqName: "k3s.cattle.io/v1, Kind=ListenerConfig_adds", help: "(Deprecated) Total number of adds handled by workqueue: k3s.cattle.io/v1, Kind=ListenerConfig", constLabels: {}, variableLabels: []} is invalid: "k3s.cattle.io/v1, Kind=ListenerConfig_adds" is not a valid metric name
E1205 13:56:52.046173    4997 prometheus.go:162] failed to register latency metric k3s.cattle.io/v1, Kind=ListenerConfig: descriptor Desc{fqName: "k3s.cattle.io/v1, Kind=ListenerConfig_queue_latency", help: "(Deprecated) How long an item stays in workqueuek3s.cattle.io/v1, Kind=ListenerConfig before being requested.", constLabels: {}, variableLabels: []} is invalid: "k3s.cattle.io/v1, Kind=ListenerConfig_queue_latency" is not a valid metric name
E1205 13:56:52.046220    4997 prometheus.go:174] failed to register work_duration metric k3s.cattle.io/v1, Kind=ListenerConfig: descriptor Desc{fqName: "k3s.cattle.io/v1, Kind=ListenerConfig_work_duration", help: "(Deprecated) How long processing an item from workqueuek3s.cattle.io/v1, Kind=ListenerConfig takes.", constLabels: {}, variableLabels: []} is invalid: "k3s.cattle.io/v1, Kind=ListenerConfig_work_duration" is not a valid metric name
E1205 13:56:52.046250    4997 prometheus.go:189] failed to register unfinished_work_seconds metric k3s.cattle.io/v1, Kind=ListenerConfig: descriptor Desc{fqName: "k3s.cattle.io/v1, Kind=ListenerConfig_unfinished_work_seconds", help: "(Deprecated) How many seconds of work k3s.cattle.io/v1, Kind=ListenerConfig has done that is in progress and hasn't been observed by work_duration. Large values indicate stuck threads. One can deduce the number of stuck threads by observing the rate at which this increases.", constLabels: {}, variableLabels: []} is invalid: "k3s.cattle.io/v1, Kind=ListenerConfig_unfinished_work_seconds" is not a valid metric name
E1205 13:56:52.046275    4997 prometheus.go:202] failed to register longest_running_processor_microseconds metric k3s.cattle.io/v1, Kind=ListenerConfig: descriptor Desc{fqName: "k3s.cattle.io/v1, Kind=ListenerConfig_longest_running_processor_microseconds", help: "(Deprecated) How many microseconds has the longest running processor for k3s.cattle.io/v1, Kind=ListenerConfig been running.", constLabels: {}, variableLabels: []} is invalid: "k3s.cattle.io/v1, Kind=ListenerConfig_longest_running_processor_microseconds" is not a valid metric name
E1205 13:56:52.046311    4997 prometheus.go:214] failed to register retries metric k3s.cattle.io/v1, Kind=ListenerConfig: descriptor Desc{fqName: "k3s.cattle.io/v1, Kind=ListenerConfig_retries", help: "(Deprecated) Total number of retries handled by workqueue: k3s.cattle.io/v1, Kind=ListenerConfig", constLabels: {}, variableLabels: []} is invalid: "k3s.cattle.io/v1, Kind=ListenerConfig_retries" is not a valid metric name
2019/12/05 13:56:52 [INFO] Waiting for k3s to start
time="2019-12-05T13:56:52.547218971Z" level=info msg="Starting k3s.cattle.io/v1, Kind=Addon controller"
time="2019-12-05T13:56:52.647400125Z" level=info msg="Starting k3s.cattle.io/v1, Kind=ListenerConfig controller"
time="2019-12-05T13:56:52.647475233Z" level=error msg="Update cert unable to convert string to cert: Unable to split cert into two parts"
time="2019-12-05T13:56:52.647731177Z" level=info msg="Node token is available at /var/lib/rancher/k3s/server/node-token"
time="2019-12-05T13:56:52.647746864Z" level=info msg="To join node to cluster: k3s agent -s https://172.17.0.2:6443 -t ${NODE_TOKEN}"
E1205 13:56:52.654810    4997 prometheus.go:138] failed to register depth metric /v1, Kind=Node: descriptor Desc{fqName: "/v1, Kind=Node_depth", help: "(Deprecated) Current depth of workqueue: /v1, Kind=Node", constLabels: {}, variableLabels: []} is invalid: "/v1, Kind=Node_depth" is not a valid metric name
E1205 13:56:52.654837    4997 prometheus.go:150] failed to register adds metric /v1, Kind=Node: descriptor Desc{fqName: "/v1, Kind=Node_adds", help: "(Deprecated) Total number of adds handled by workqueue: /v1, Kind=Node", constLabels: {}, variableLabels: []} is invalid: "/v1, Kind=Node_adds" is not a valid metric name
E1205 13:56:52.654886    4997 prometheus.go:162] failed to register latency metric /v1, Kind=Node: descriptor Desc{fqName: "/v1, Kind=Node_queue_latency", help: "(Deprecated) How long an item stays in workqueue/v1, Kind=Node before being requested.", constLabels: {}, variableLabels: []} is invalid: "/v1, Kind=Node_queue_latency" is not a valid metric name
E1205 13:56:52.654917    4997 prometheus.go:174] failed to register work_duration metric /v1, Kind=Node: descriptor Desc{fqName: "/v1, Kind=Node_work_duration", help: "(Deprecated) How long processing an item from workqueue/v1, Kind=Node takes.", constLabels: {}, variableLabels: []} is invalid: "/v1, Kind=Node_work_duration" is not a valid metric name
E1205 13:56:52.654937    4997 prometheus.go:189] failed to register unfinished_work_seconds metric /v1, Kind=Node: descriptor Desc{fqName: "/v1, Kind=Node_unfinished_work_seconds", help: "(Deprecated) How many seconds of work /v1, Kind=Node has done that is in progress and hasn't been observed by work_duration. Large values indicate stuck threads. One can deduce the number of stuck threads by observing the rate at which this increases.", constLabels: {}, variableLabels: []} is invalid: "/v1, Kind=Node_unfinished_work_seconds" is not a valid metric name
E1205 13:56:52.654953    4997 prometheus.go:202] failed to register longest_running_processor_microseconds metric /v1, Kind=Node: descriptor Desc{fqName: "/v1, Kind=Node_longest_running_processor_microseconds", help: "(Deprecated) How many microseconds has the longest running processor for /v1, Kind=Node been running.", constLabels: {}, variableLabels: []} is invalid: "/v1, Kind=Node_longest_running_processor_microseconds" is not a valid metric name
E1205 13:56:52.654980    4997 prometheus.go:214] failed to register retries metric /v1, Kind=Node: descriptor Desc{fqName: "/v1, Kind=Node_retries", help: "(Deprecated) Total number of retries handled by workqueue: /v1, Kind=Node", constLabels: {}, variableLabels: []} is invalid: "/v1, Kind=Node_retries" is not a valid metric name
E1205 13:56:52.655096    4997 prometheus.go:138] failed to register depth metric batch/v1, Kind=Job: descriptor Desc{fqName: "batch/v1, Kind=Job_depth", help: "(Deprecated) Current depth of workqueue: batch/v1, Kind=Job", constLabels: {}, variableLabels: []} is invalid: "batch/v1, Kind=Job_depth" is not a valid metric name
E1205 13:56:52.655114    4997 prometheus.go:150] failed to register adds metric batch/v1, Kind=Job: descriptor Desc{fqName: "batch/v1, Kind=Job_adds", help: "(Deprecated) Total number of adds handled by workqueue: batch/v1, Kind=Job", constLabels: {}, variableLabels: []} is invalid: "batch/v1, Kind=Job_adds" is not a valid metric name
E1205 13:56:52.655140    4997 prometheus.go:162] failed to register latency metric batch/v1, Kind=Job: descriptor Desc{fqName: "batch/v1, Kind=Job_queue_latency", help: "(Deprecated) How long an item stays in workqueuebatch/v1, Kind=Job before being requested.", constLabels: {}, variableLabels: []} is invalid: "batch/v1, Kind=Job_queue_latency" is not a valid metric name
E1205 13:56:52.655163    4997 prometheus.go:174] failed to register work_duration metric batch/v1, Kind=Job: descriptor Desc{fqName: "batch/v1, Kind=Job_work_duration", help: "(Deprecated) How long processing an item from workqueuebatch/v1, Kind=Job takes.", constLabels: {}, variableLabels: []} is invalid: "batch/v1, Kind=Job_work_duration" is not a valid metric name
E1205 13:56:52.655198    4997 prometheus.go:189] failed to register unfinished_work_seconds metric batch/v1, Kind=Job: descriptor Desc{fqName: "batch/v1, Kind=Job_unfinished_work_seconds", help: "(Deprecated) How many seconds of work batch/v1, Kind=Job has done that is in progress and hasn't been observed by work_duration. Large values indicate stuck threads. One can deduce the number of stuck threads by observing the rate at which this increases.", constLabels: {}, variableLabels: []} is invalid: "batch/v1, Kind=Job_unfinished_work_seconds" is not a valid metric name
E1205 13:56:52.655217    4997 prometheus.go:202] failed to register longest_running_processor_microseconds metric batch/v1, Kind=Job: descriptor Desc{fqName: "batch/v1, Kind=Job_longest_running_processor_microseconds", help: "(Deprecated) How many microseconds has the longest running processor for batch/v1, Kind=Job been running.", constLabels: {}, variableLabels: []} is invalid: "batch/v1, Kind=Job_longest_running_processor_microseconds" is not a valid metric name
E1205 13:56:52.655248    4997 prometheus.go:214] failed to register retries metric batch/v1, Kind=Job: descriptor Desc{fqName: "batch/v1, Kind=Job_retries", help: "(Deprecated) Total number of retries handled by workqueue: batch/v1, Kind=Job", constLabels: {}, variableLabels: []} is invalid: "batch/v1, Kind=Job_retries" is not a valid metric name
E1205 13:56:52.655330    4997 prometheus.go:138] failed to register depth metric helm.cattle.io/v1, Kind=HelmChart: descriptor Desc{fqName: "helm.cattle.io/v1, Kind=HelmChart_depth", help: "(Deprecated) Current depth of workqueue: helm.cattle.io/v1, Kind=HelmChart", constLabels: {}, variableLabels: []} is invalid: "helm.cattle.io/v1, Kind=HelmChart_depth" is not a valid metric name
E1205 13:56:52.655367    4997 prometheus.go:150] failed to register adds metric helm.cattle.io/v1, Kind=HelmChart: descriptor Desc{fqName: "helm.cattle.io/v1, Kind=HelmChart_adds", help: "(Deprecated) Total number of adds handled by workqueue: helm.cattle.io/v1, Kind=HelmChart", constLabels: {}, variableLabels: []} is invalid: "helm.cattle.io/v1, Kind=HelmChart_adds" is not a valid metric name
E1205 13:56:52.655396    4997 prometheus.go:162] failed to register latency metric helm.cattle.io/v1, Kind=HelmChart: descriptor Desc{fqName: "helm.cattle.io/v1, Kind=HelmChart_queue_latency", help: "(Deprecated) How long an item stays in workqueuehelm.cattle.io/v1, Kind=HelmChart before being requested.", constLabels: {}, variableLabels: []} is invalid: "helm.cattle.io/v1, Kind=HelmChart_queue_latency" is not a valid metric name
E1205 13:56:52.655417    4997 prometheus.go:174] failed to register work_duration metric helm.cattle.io/v1, Kind=HelmChart: descriptor Desc{fqName: "helm.cattle.io/v1, Kind=HelmChart_work_duration", help: "(Deprecated) How long processing an item from workqueuehelm.cattle.io/v1, Kind=HelmChart takes.", constLabels: {}, variableLabels: []} is invalid: "helm.cattle.io/v1, Kind=HelmChart_work_duration" is not a valid metric name
E1205 13:56:52.655436    4997 prometheus.go:189] failed to register unfinished_work_seconds metric helm.cattle.io/v1, Kind=HelmChart: descriptor Desc{fqName: "helm.cattle.io/v1, Kind=HelmChart_unfinished_work_seconds", help: "(Deprecated) How many seconds of work helm.cattle.io/v1, Kind=HelmChart has done that is in progress and hasn't been observed by work_duration. Large values indicate stuck threads. One can deduce the number of stuck threads by observing the rate at which this increases.", constLabels: {}, variableLabels: []} is invalid: "helm.cattle.io/v1, Kind=HelmChart_unfinished_work_seconds" is not a valid metric name
E1205 13:56:52.655452    4997 prometheus.go:202] failed to register longest_running_processor_microseconds metric helm.cattle.io/v1, Kind=HelmChart: descriptor Desc{fqName: "helm.cattle.io/v1, Kind=HelmChart_longest_running_processor_microseconds", help: "(Deprecated) How many microseconds has the longest running processor for helm.cattle.io/v1, Kind=HelmChart been running.", constLabels: {}, variableLabels: []} is invalid: "helm.cattle.io/v1, Kind=HelmChart_longest_running_processor_microseconds" is not a valid metric name
E1205 13:56:52.655476    4997 prometheus.go:214] failed to register retries metric helm.cattle.io/v1, Kind=HelmChart: descriptor Desc{fqName: "helm.cattle.io/v1, Kind=HelmChart_retries", help: "(Deprecated) Total number of retries handled by workqueue: helm.cattle.io/v1, Kind=HelmChart", constLabels: {}, variableLabels: []} is invalid: "helm.cattle.io/v1, Kind=HelmChart_retries" is not a valid metric name
E1205 13:56:52.655583    4997 prometheus.go:138] failed to register depth metric /v1, Kind=Service: descriptor Desc{fqName: "/v1, Kind=Service_depth", help: "(Deprecated) Current depth of workqueue: /v1, Kind=Service", constLabels: {}, variableLabels: []} is invalid: "/v1, Kind=Service_depth" is not a valid metric name
E1205 13:56:52.655606    4997 prometheus.go:150] failed to register adds metric /v1, Kind=Service: descriptor Desc{fqName: "/v1, Kind=Service_adds", help: "(Deprecated) Total number of adds handled by workqueue: /v1, Kind=Service", constLabels: {}, variableLabels: []} is invalid: "/v1, Kind=Service_adds" is not a valid metric name
E1205 13:56:52.655629    4997 prometheus.go:162] failed to register latency metric /v1, Kind=Service: descriptor Desc{fqName: "/v1, Kind=Service_queue_latency", help: "(Deprecated) How long an item stays in workqueue/v1, Kind=Service before being requested.", constLabels: {}, variableLabels: []} is invalid: "/v1, Kind=Service_queue_latency" is not a valid metric name
E1205 13:56:52.655648    4997 prometheus.go:174] failed to register work_duration metric /v1, Kind=Service: descriptor Desc{fqName: "/v1, Kind=Service_work_duration", help: "(Deprecated) How long processing an item from workqueue/v1, Kind=Service takes.", constLabels: {}, variableLabels: []} is invalid: "/v1, Kind=Service_work_duration" is not a valid metric name
E1205 13:56:52.655666    4997 prometheus.go:189] failed to register unfinished_work_seconds metric /v1, Kind=Service: descriptor Desc{fqName: "/v1, Kind=Service_unfinished_work_seconds", help: "(Deprecated) How many seconds of work /v1, Kind=Service has done that is in progress and hasn't been observed by work_duration. Large values indicate stuck threads. One can deduce the number of stuck threads by observing the rate at which this increases.", constLabels: {}, variableLabels: []} is invalid: "/v1, Kind=Service_unfinished_work_seconds" is not a valid metric name
E1205 13:56:52.655702    4997 prometheus.go:202] failed to register longest_running_processor_microseconds metric /v1, Kind=Service: descriptor Desc{fqName: "/v1, Kind=Service_longest_running_processor_microseconds", help: "(Deprecated) How many microseconds has the longest running processor for /v1, Kind=Service been running.", constLabels: {}, variableLabels: []} is invalid: "/v1, Kind=Service_longest_running_processor_microseconds" is not a valid metric name
E1205 13:56:52.655728    4997 prometheus.go:214] failed to register retries metric /v1, Kind=Service: descriptor Desc{fqName: "/v1, Kind=Service_retries", help: "(Deprecated) Total number of retries handled by workqueue: /v1, Kind=Service", constLabels: {}, variableLabels: []} is invalid: "/v1, Kind=Service_retries" is not a valid metric name
E1205 13:56:52.655779    4997 prometheus.go:138] failed to register depth metric /v1, Kind=Pod: descriptor Desc{fqName: "/v1, Kind=Pod_depth", help: "(Deprecated) Current depth of workqueue: /v1, Kind=Pod", constLabels: {}, variableLabels: []} is invalid: "/v1, Kind=Pod_depth" is not a valid metric name
E1205 13:56:52.655794    4997 prometheus.go:150] failed to register adds metric /v1, Kind=Pod: descriptor Desc{fqName: "/v1, Kind=Pod_adds", help: "(Deprecated) Total number of adds handled by workqueue: /v1, Kind=Pod", constLabels: {}, variableLabels: []} is invalid: "/v1, Kind=Pod_adds" is not a valid metric name
E1205 13:56:52.655819    4997 prometheus.go:162] failed to register latency metric /v1, Kind=Pod: descriptor Desc{fqName: "/v1, Kind=Pod_queue_latency", help: "(Deprecated) How long an item stays in workqueue/v1, Kind=Pod before being requested.", constLabels: {}, variableLabels: []} is invalid: "/v1, Kind=Pod_queue_latency" is not a valid metric name
E1205 13:56:52.655855    4997 prometheus.go:174] failed to register work_duration metric /v1, Kind=Pod: descriptor Desc{fqName: "/v1, Kind=Pod_work_duration", help: "(Deprecated) How long processing an item from workqueue/v1, Kind=Pod takes.", constLabels: {}, variableLabels: []} is invalid: "/v1, Kind=Pod_work_duration" is not a valid metric name
E1205 13:56:52.655876    4997 prometheus.go:189] failed to register unfinished_work_seconds metric /v1, Kind=Pod: descriptor Desc{fqName: "/v1, Kind=Pod_unfinished_work_seconds", help: "(Deprecated) How many seconds of work /v1, Kind=Pod has done that is in progress and hasn't been observed by work_duration. Large values indicate stuck threads. One can deduce the number of stuck threads by observing the rate at which this increases.", constLabels: {}, variableLabels: []} is invalid: "/v1, Kind=Pod_unfinished_work_seconds" is not a valid metric name
E1205 13:56:52.655896    4997 prometheus.go:202] failed to register longest_running_processor_microseconds metric /v1, Kind=Pod: descriptor Desc{fqName: "/v1, Kind=Pod_longest_running_processor_microseconds", help: "(Deprecated) How many microseconds has the longest running processor for /v1, Kind=Pod been running.", constLabels: {}, variableLabels: []} is invalid: "/v1, Kind=Pod_longest_running_processor_microseconds" is not a valid metric name
E1205 13:56:52.655919    4997 prometheus.go:214] failed to register retries metric /v1, Kind=Pod: descriptor Desc{fqName: "/v1, Kind=Pod_retries", help: "(Deprecated) Total number of retries handled by workqueue: /v1, Kind=Pod", constLabels: {}, variableLabels: []} is invalid: "/v1, Kind=Pod_retries" is not a valid metric name
E1205 13:56:52.655969    4997 prometheus.go:138] failed to register depth metric /v1, Kind=Endpoints: descriptor Desc{fqName: "/v1, Kind=Endpoints_depth", help: "(Deprecated) Current depth of workqueue: /v1, Kind=Endpoints", constLabels: {}, variableLabels: []} is invalid: "/v1, Kind=Endpoints_depth" is not a valid metric name
E1205 13:56:52.655983    4997 prometheus.go:150] failed to register adds metric /v1, Kind=Endpoints: descriptor Desc{fqName: "/v1, Kind=Endpoints_adds", help: "(Deprecated) Total number of adds handled by workqueue: /v1, Kind=Endpoints", constLabels: {}, variableLabels: []} is invalid: "/v1, Kind=Endpoints_adds" is not a valid metric name
E1205 13:56:52.656048    4997 prometheus.go:162] failed to register latency metric /v1, Kind=Endpoints: descriptor Desc{fqName: "/v1, Kind=Endpoints_queue_latency", help: "(Deprecated) How long an item stays in workqueue/v1, Kind=Endpoints before being requested.", constLabels: {}, variableLabels: []} is invalid: "/v1, Kind=Endpoints_queue_latency" is not a valid metric name
E1205 13:56:52.656072    4997 prometheus.go:174] failed to register work_duration metric /v1, Kind=Endpoints: descriptor Desc{fqName: "/v1, Kind=Endpoints_work_duration", help: "(Deprecated) How long processing an item from workqueue/v1, Kind=Endpoints takes.", constLabels: {}, variableLabels: []} is invalid: "/v1, Kind=Endpoints_work_duration" is not a valid metric name
E1205 13:56:52.656091    4997 prometheus.go:189] failed to register unfinished_work_seconds metric /v1, Kind=Endpoints: descriptor Desc{fqName: "/v1, Kind=Endpoints_unfinished_work_seconds", help: "(Deprecated) How many seconds of work /v1, Kind=Endpoints has done that is in progress and hasn't been observed by work_duration. Large values indicate stuck threads. One can deduce the number of stuck threads by observing the rate at which this increases.", constLabels: {}, variableLabels: []} is invalid: "/v1, Kind=Endpoints_unfinished_work_seconds" is not a valid metric name
E1205 13:56:52.656106    4997 prometheus.go:202] failed to register longest_running_processor_microseconds metric /v1, Kind=Endpoints: descriptor Desc{fqName: "/v1, Kind=Endpoints_longest_running_processor_microseconds", help: "(Deprecated) How many microseconds has the longest running processor for /v1, Kind=Endpoints been running.", constLabels: {}, variableLabels: []} is invalid: "/v1, Kind=Endpoints_longest_running_processor_microseconds" is not a valid metric name
E1205 13:56:52.656129    4997 prometheus.go:214] failed to register retries metric /v1, Kind=Endpoints: descriptor Desc{fqName: "/v1, Kind=Endpoints_retries", help: "(Deprecated) Total number of retries handled by workqueue: /v1, Kind=Endpoints", constLabels: {}, variableLabels: []} is invalid: "/v1, Kind=Endpoints_retries" is not a valid metric name
time="2019-12-05T13:56:52.752201194Z" level=info msg="Wrote kubeconfig /etc/rancher/k3s/k3s.yaml"
time="2019-12-05T13:56:52.752225694Z" level=info msg="Run: k3s kubectl"
time="2019-12-05T13:56:52.752234086Z" level=info msg="k3s is up and running"
2019/12/05 13:56:53 [INFO] Running in single server mode, will not peer connections
2019/12/05 13:56:53 [INFO] Creating CRD apps.project.cattle.io
2019/12/05 13:56:53 [INFO] Creating CRD authconfigs.management.cattle.io
2019/12/05 13:56:53 [INFO] Creating CRD catalogs.management.cattle.io
2019/12/05 13:56:53 [INFO] Creating CRD apprevisions.project.cattle.io
2019/12/05 13:56:53 [INFO] Creating CRD pipelineexecutions.project.cattle.io
2019/12/05 13:56:53 [INFO] Creating CRD catalogtemplates.management.cattle.io
2019/12/05 13:56:53 [INFO] Creating CRD catalogtemplateversions.management.cattle.io
2019/12/05 13:56:53 [INFO] Creating CRD pipelinesettings.project.cattle.io
2019/12/05 13:56:53 [INFO] Creating CRD pipelines.project.cattle.io
2019/12/05 13:56:53 [INFO] Creating CRD clusteralerts.management.cattle.io
2019/12/05 13:56:53 [INFO] Creating CRD sourcecodecredentials.project.cattle.io
2019/12/05 13:56:53 [INFO] Creating CRD clusteralertgroups.management.cattle.io
time="2019-12-05T13:56:53.661515605Z" level=info msg="Starting helm.cattle.io/v1, Kind=HelmChart controller"
2019/12/05 13:56:53 [INFO] Creating CRD sourcecodeproviderconfigs.project.cattle.io
W1205 13:56:53.843111    4997 lease.go:222] Resetting endpoints for master service "kubernetes" to [172.17.0.2]
2019/12/05 13:56:54 [INFO] Creating CRD clustercatalogs.management.cattle.io
time="2019-12-05T13:56:54.162487032Z" level=info msg="Starting batch/v1, Kind=Job controller"
2019/12/05 13:56:54 [INFO] Creating CRD sourcecoderepositories.project.cattle.io
2019/12/05 13:56:54 [INFO] Creating CRD clusterloggings.management.cattle.io
2019/12/05 13:56:54 [INFO] Creating CRD clusteralertrules.management.cattle.io
time="2019-12-05T13:56:54.864372147Z" level=info msg="Starting /v1, Kind=Endpoints controller"
time="2019-12-05T13:56:54.964564459Z" level=info msg="Starting /v1, Kind=Node controller"
time="2019-12-05T13:56:55.065680958Z" level=info msg="Starting /v1, Kind=Service controller"
time="2019-12-05T13:56:55.165925844Z" level=info msg="Starting /v1, Kind=Pod controller"
2019/12/05 13:56:55 [INFO] Creating CRD clustermonitorgraphs.management.cattle.io
2019/12/05 13:56:55 [INFO] Creating CRD clusterregistrationtokens.management.cattle.io
2019/12/05 13:56:55 [INFO] Creating CRD clusterroletemplatebindings.management.cattle.io
2019/12/05 13:56:55 [INFO] Creating CRD clusterscans.management.cattle.io
2019/12/05 13:56:56 [INFO] Creating CRD clusters.management.cattle.io
2019/12/05 13:56:56 [INFO] Creating CRD composeconfigs.management.cattle.io
2019/12/05 13:56:56 [INFO] Creating CRD dynamicschemas.management.cattle.io
2019/12/05 13:56:56 [INFO] Creating CRD etcdbackups.management.cattle.io
2019/12/05 13:56:56 [INFO] Creating CRD features.management.cattle.io
2019/12/05 13:56:57 [INFO] Creating CRD globalrolebindings.management.cattle.io
2019/12/05 13:56:57 [INFO] Creating CRD globalroles.management.cattle.io
2019/12/05 13:56:57 [INFO] Creating CRD groupmembers.management.cattle.io
2019/12/05 13:56:57 [INFO] Creating CRD groups.management.cattle.io
2019/12/05 13:56:57 [INFO] Creating CRD kontainerdrivers.management.cattle.io
2019/12/05 13:56:58 [INFO] Creating CRD listenconfigs.management.cattle.io
2019/12/05 13:56:58 [INFO] Creating CRD multiclusterapps.management.cattle.io
2019/12/05 13:56:58 [INFO] Creating CRD multiclusterapprevisions.management.cattle.io
2019/12/05 13:56:58 [INFO] Creating CRD monitormetrics.management.cattle.io
2019/12/05 13:56:58 [INFO] Creating CRD nodedrivers.management.cattle.io
2019/12/05 13:56:59 [INFO] Creating CRD nodepools.management.cattle.io
2019/12/05 13:56:59 [INFO] Creating CRD nodetemplates.management.cattle.io
2019/12/05 13:56:59 [INFO] Creating CRD nodes.management.cattle.io
2019/12/05 13:56:59 [INFO] Creating CRD notifiers.management.cattle.io
2019/12/05 13:56:59 [INFO] Creating CRD podsecuritypolicytemplateprojectbindings.management.cattle.io
2019/12/05 13:57:00 [INFO] Creating CRD podsecuritypolicytemplates.management.cattle.io
2019/12/05 13:57:00 [INFO] Creating CRD preferences.management.cattle.io
2019/12/05 13:57:00 [INFO] Creating CRD projectalerts.management.cattle.io
2019/12/05 13:57:00 [INFO] Creating CRD projectalertgroups.management.cattle.io
2019/12/05 13:57:00 [INFO] Creating CRD projectcatalogs.management.cattle.io
2019/12/05 13:57:01 [INFO] Creating CRD projectloggings.management.cattle.io
2019/12/05 13:57:01 [INFO] Creating CRD projectalertrules.management.cattle.io
2019/12/05 13:57:01 [INFO] Creating CRD projectmonitorgraphs.management.cattle.io
2019/12/05 13:57:01 [INFO] Creating CRD projectnetworkpolicies.management.cattle.io
2019/12/05 13:57:01 [INFO] Creating CRD projectroletemplatebindings.management.cattle.io
2019/12/05 13:57:02 [INFO] Creating CRD projects.management.cattle.io
2019/12/05 13:57:02 [INFO] Creating CRD rkek8ssystemimages.management.cattle.io
2019/12/05 13:57:02 [INFO] Creating CRD rkek8sserviceoptions.management.cattle.io
2019/12/05 13:57:02 [INFO] Creating CRD rkeaddons.management.cattle.io
2019/12/05 13:57:02 [INFO] Creating CRD roletemplates.management.cattle.io
2019/12/05 13:57:03 [INFO] Creating CRD settings.management.cattle.io
2019/12/05 13:57:03 [INFO] Creating CRD templates.management.cattle.io
2019/12/05 13:57:03 [INFO] Creating CRD templateversions.management.cattle.io
2019/12/05 13:57:03 [INFO] Creating CRD templatecontents.management.cattle.io
2019/12/05 13:57:03 [INFO] Creating CRD tokens.management.cattle.io
2019/12/05 13:57:04 [INFO] Creating CRD userattributes.management.cattle.io
2019/12/05 13:57:04 [INFO] Creating CRD users.management.cattle.io
2019/12/05 13:57:04 [INFO] Creating CRD globaldnses.management.cattle.io
2019/12/05 13:57:04 [INFO] Creating CRD globaldnsproviders.management.cattle.io
2019/12/05 13:57:04 [INFO] Creating CRD clustertemplates.management.cattle.io
2019/12/05 13:57:05 [INFO] Creating CRD clustertemplaterevisions.management.cattle.io
2019/12/05 13:57:05 http: TLS handshake error from 127.0.0.1:53776: EOF
W1205 13:57:05.581927    4997 controllermanager.go:445] Skipping "root-ca-cert-publisher"
2019/12/05 13:57:06 [INFO] Starting API controllers
2019/12/05 13:57:06 [INFO] Starting catalog controller
2019/12/05 13:57:06 [INFO] Starting project-level catalog controller
2019/12/05 13:57:06 [INFO] Starting cluster-level catalog controller
2019/12/05 13:57:06 [INFO] Starting management controllers
2019/12/05 13:57:06 [INFO] Listening on :8443
2019/12/05 13:57:06 [INFO] Listening on :8080
2019/12/05 13:57:06 [INFO] Reconciling GlobalRoles
2019/12/05 13:57:06 [INFO] Creating clusters-create
2019/12/05 13:57:06 [INFO] Creating users-manage
2019/12/05 13:57:06 [INFO] Creating clustertemplates-create
2019/12/05 13:57:06 [INFO] Creating user
2019/12/05 13:57:06 [INFO] [mgmt-auth-gr-controller] Creating clusterRole cattle-globalrole-users-manage for corresponding GlobalRole
2019/12/05 13:57:06 [INFO] [mgmt-auth-gr-controller] Creating clusterRole cattle-globalrole-clusters-create for corresponding GlobalRole
2019/12/05 13:57:06 [INFO] Creating user-base
2019/12/05 13:57:06 [INFO] [mgmt-auth-gr-controller] Creating clusterRole cattle-globalrole-clustertemplates-create for corresponding GlobalRole
2019/12/05 13:57:06 [INFO] [mgmt-auth-gr-controller] Creating clusterRole cattle-globalrole-user for corresponding GlobalRole
2019/12/05 13:57:06 [INFO] Creating catalogs-use
2019/12/05 13:57:06 [INFO] Creating roles-manage
2019/12/05 13:57:06 [INFO] [mgmt-auth-gr-controller] Creating clusterRole cattle-globalrole-user-base for corresponding GlobalRole
2019/12/05 13:57:06 [INFO] [mgmt-auth-gr-controller] Creating clusterRole cattle-globalrole-catalogs-use for corresponding GlobalRole
2019/12/05 13:57:06 [INFO] Creating settings-manage
2019/12/05 13:57:06 [INFO] [mgmt-auth-gr-controller] Creating clusterRole cattle-globalrole-roles-manage for corresponding GlobalRole
2019/12/05 13:57:06 [INFO] Creating nodedrivers-manage
2019/12/05 13:57:06 [INFO] Creating podsecuritypolicytemplates-manage
2019/12/05 13:57:06 [INFO] [mgmt-auth-gr-controller] Creating clusterRole cattle-globalrole-settings-manage for corresponding GlobalRole
2019/12/05 13:57:06 [INFO] Creating admin
2019/12/05 13:57:06 [INFO] [mgmt-auth-gr-controller] Creating clusterRole cattle-globalrole-nodedrivers-manage for corresponding GlobalRole
2019/12/05 13:57:06 [INFO] [mgmt-auth-gr-controller] Creating clusterRole cattle-globalrole-podsecuritypolicytemplates-manage for corresponding GlobalRole
2019/12/05 13:57:06 [INFO] Creating kontainerdrivers-manage
2019/12/05 13:57:06 [INFO] Creating catalogs-manage
2019/12/05 13:57:06 [INFO] [mgmt-auth-gr-controller] Creating clusterRole cattle-globalrole-admin for corresponding GlobalRole
2019/12/05 13:57:06 [INFO] [mgmt-auth-gr-controller] Creating clusterRole cattle-globalrole-kontainerdrivers-manage for corresponding GlobalRole
2019/12/05 13:57:06 [INFO] Creating authn-manage
2019/12/05 13:57:06 [INFO] Creating features-manage
2019/12/05 13:57:06 [INFO] Reconciling RoleTemplates
2019/12/05 13:57:06 [INFO] [mgmt-auth-gr-controller] Creating clusterRole cattle-globalrole-catalogs-manage for corresponding GlobalRole
2019/12/05 13:57:06 [INFO] [mgmt-auth-gr-controller] Creating clusterRole cattle-globalrole-authn-manage for corresponding GlobalRole
2019/12/05 13:57:06 [INFO] Creating clustercatalogs-view
2019/12/05 13:57:06 [INFO] Creating secrets-manage
2019/12/05 13:57:06 [INFO] [mgmt-auth-gr-controller] Creating clusterRole cattle-globalrole-features-manage for corresponding GlobalRole
2019/12/05 13:57:06 [INFO] Creating projectroletemplatebindings-manage
2019/12/05 13:57:06 [INFO] Creating projectcatalogs-manage
2019/12/05 13:57:06 [INFO] Creating cluster-admin
2019/12/05 13:57:06 [INFO] Creating clustercatalogs-manage
2019/12/05 13:57:06 [INFO] Creating project-monitoring-readonly
2019/12/05 13:57:06 [INFO] Creating read-only
2019/12/05 13:57:06 [INFO] Creating workloads-view
2019/12/05 13:57:06 [INFO] Creating workloads-manage
2019/12/05 13:57:06 [INFO] Creating view
2019/12/05 13:57:06 [INFO] Creating storage-manage
2019/12/05 13:57:06 [INFO] Creating nodes-view
2019/12/05 13:57:06 [INFO] Creating clusterroletemplatebindings-manage
2019/12/05 13:57:06 [INFO] Creating clusterroletemplatebindings-view
2019/12/05 13:57:06 [INFO] Creating clusterscans-manage
2019/12/05 13:57:06 [INFO] Creating ingress-manage
2019/12/05 13:57:06 [INFO] Creating configmaps-manage
2019/12/05 13:57:06 [INFO] Creating admin
2019/12/05 13:57:06 [INFO] Creating nodes-manage
2019/12/05 13:57:06 [INFO] Creating persistentvolumeclaims-view
2019/12/05 13:57:06 [INFO] Creating projectroletemplatebindings-view
2019/12/05 13:57:06 [INFO] Creating create-ns
2019/12/05 13:57:07 [INFO] Creating serviceaccounts-manage
2019/12/05 13:57:07 [INFO] Creating project-owner
2019/12/05 13:57:07 [INFO] Creating project-member
2019/12/05 13:57:07 [INFO] Creating ingress-view
2019/12/05 13:57:07 [INFO] Creating secrets-view
2019/12/05 13:57:07 [INFO] Creating persistentvolumeclaims-manage
2019/12/05 13:57:07 [INFO] Creating serviceaccounts-view
2019/12/05 13:57:07 [INFO] Creating projectcatalogs-view
2019/12/05 13:57:07 [INFO] Creating cluster-member
2019/12/05 13:57:07 [INFO] Creating services-manage
2019/12/05 13:57:07 [INFO] Creating projects-create
2019/12/05 13:57:07 [INFO] Creating projects-view
2019/12/05 13:57:07 [INFO] Creating backups-manage
2019/12/05 13:57:07 [INFO] Creating services-view
2019/12/05 13:57:07 [INFO] Creating configmaps-view
2019/12/05 13:57:07 [INFO] Creating edit
2019/12/05 13:57:07 [INFO] Creating cluster-owner
2019/12/05 13:57:07 [INFO] Created default admin user and binding
2019/12/05 13:57:07 [INFO] Creating new GlobalRoleBinding for GlobalRoleBinding globalrolebinding-nwspc
2019/12/05 13:57:07 [INFO] [mgmt-auth-grb-controller] Creating clusterRoleBinding for globalRoleBinding globalrolebinding-nwspc for user user-bvqq8 with role cattle-globalrole-admin
2019/12/05 13:57:07 [INFO] [mgmt-cluster-rbac-delete] Creating namespace local
2019/12/05 13:57:07 [INFO] [mgmt-cluster-rbac-delete] Creating Default project for cluster local
2019/12/05 13:57:07 [INFO] adding kontainer driver rancherKubernetesEngine
2019/12/05 13:57:07 [INFO] adding kontainer driver googleKubernetesEngine
2019/12/05 13:57:07 [INFO] adding kontainer driver azureKubernetesService
2019/12/05 13:57:07 [INFO] create kontainerdriver rancherkubernetesengine
2019/12/05 13:57:07 [INFO] [mgmt-project-rbac-create] Creating namespace p-vdv4s
2019/12/05 13:57:07 [INFO] [mgmt-cluster-rbac-delete] Creating System project for cluster local
2019/12/05 13:57:07 [INFO] [mgmt-project-rbac-create] Creating creator projectRoleTemplateBinding for user user-bvqq8 for project p-vdv4s
2019/12/05 13:57:07 [INFO] [mgmt-project-rbac-create] Creating namespace p-znlft
2019/12/05 13:57:07 [INFO] [mgmt-cluster-rbac-delete] Updating cluster local
2019/12/05 13:57:07 [INFO] adding kontainer driver amazonElasticContainerService
2019/12/05 13:57:07 [INFO] create kontainerdriver googlekubernetesengine
2019/12/05 13:57:07 [INFO] [mgmt-project-rbac-create] Creating creator clusterRoleTemplateBinding for user user-bvqq8 for cluster local
2019/12/05 13:57:07 [INFO] [mgmt-auth-crtb-controller] Setting InitialRolesPopulated condition on project p-vdv4s
2019/12/05 13:57:07 [INFO] create kontainerdriver azurekubernetesservice
2019/12/05 13:57:07 [INFO] adding kontainer driver baiducloudcontainerengine
2019/12/05 13:57:07 [INFO] [mgmt-project-rbac-create] Creating creator projectRoleTemplateBinding for user user-bvqq8 for project p-znlft
2019/12/05 13:57:07 [INFO] create kontainerdriver rancherkubernetesengine
2019/12/05 13:57:07 [INFO] [mgmt-project-rbac-create] Updating project p-vdv4s
2019/12/05 13:57:07 [INFO] [mgmt-auth-crtb-controller] Creating clusterRole local-clusterowner
2019/12/05 13:57:07 [INFO] [mgmt-auth-prtb-controller] Creating clusterRole p-vdv4s-projectowner
2019/12/05 13:57:07 [INFO] [mgmt-auth-crtb-controller] Setting InitialRolesPopulated condition on project p-znlft
2019/12/05 13:57:07 [INFO] adding kontainer driver aliyunkubernetescontainerservice
2019/12/05 13:57:07 [INFO] update kontainerdriver googlekubernetesengine
2019/12/05 13:57:07 [INFO] update kontainerdriver rancherkubernetesengine
2019/12/05 13:57:07 [INFO] [mgmt-auth-crtb-controller] Setting InitialRolesPopulated condition on cluster 
2019/12/05 13:57:07 [INFO] [mgmt-cluster-rbac-delete] Updating cluster local
2019/12/05 13:57:07 [INFO] [mgmt-auth-prtb-controller] Creating clusterRole p-znlft-projectowner
2019/12/05 13:57:07 [INFO] adding kontainer driver tencentkubernetesengine
2019/12/05 13:57:07 [INFO] create kontainerdriver amazonelasticcontainerservice
2019/12/05 13:57:07 [INFO] create kontainerdriver baiducloudcontainerengine
2019/12/05 13:57:07 [INFO] [mgmt-project-rbac-create] Updating project p-znlft
2019/12/05 13:57:07 [INFO] create kontainerdriver aliyunkubernetescontainerservice
2019/12/05 13:57:07 [INFO] create kontainerdriver azurekubernetesservice
2019/12/05 13:57:07 [INFO] adding kontainer driver huaweicontainercloudengine
2019/12/05 13:57:07 [INFO] [mgmt-auth-crtb-controller] Creating clusterRoleBinding for membership in cluster local for subject user-bvqq8
2019/12/05 13:57:07 [INFO] [mgmt-project-rbac-create] Updating project p-vdv4s
2019/12/05 13:57:07 [INFO] update kontainerdriver azurekubernetesservice
2019/12/05 13:57:07 [INFO] update kontainerdriver amazonelasticcontainerservice
2019/12/05 13:57:07 [INFO] create kontainerdriver baiducloudcontainerengine
2019/12/05 13:57:07 [INFO] [mgmt-auth-crtb-controller] Creating role cluster-owner in namespace local
2019/12/05 13:57:07 [INFO] [mgmt-project-rbac-create] Updating project p-znlft
2019/12/05 13:57:07 [INFO] [mgmt-auth-prtb-controller] Creating roleBinding for membership in project p-znlft for subject user-bvqq8
2019/12/05 13:57:07 [INFO] [mgmt-auth-prtb-controller] Creating roleBinding for membership in project p-vdv4s for subject user-bvqq8
2019/12/05 13:57:07 [INFO] update kontainerdriver baiducloudcontainerengine
2019/12/05 13:57:07 [INFO] create kontainerdriver tencentkubernetesengine
2019/12/05 13:57:07 [INFO] [mgmt-auth-prtb-controller] Creating clusterRole local-clustermember
2019/12/05 13:57:07 [INFO] [mgmt-auth-crtb-controller] Creating roleBinding for subject user-bvqq8 with role cluster-owner in namespace 
2019/12/05 13:57:07 [INFO] Created cattle-global-nt namespace
2019/12/05 13:57:07 [INFO] Creating node driver pinganyunecs
2019/12/05 13:57:07 [INFO] create kontainerdriver huaweicontainercloudengine
2019/12/05 13:57:07 [INFO] create kontainerdriver aliyunkubernetescontainerservice
2019/12/05 13:57:07 [INFO] [mgmt-auth-prtb-controller] Creating clusterRole local-clustermember
2019/12/05 13:57:07 [INFO] update kontainerdriver tencentkubernetesengine
2019/12/05 13:57:07 [INFO] update kontainerdriver aliyunkubernetescontainerservice
2019/12/05 13:57:07 [INFO] [mgmt-auth-prtb-controller] Creating clusterRoleBinding for membership in cluster local for subject user-bvqq8
2019/12/05 13:57:07 [INFO] [mgmt-auth-crtb-controller] Creating role cluster-owner in namespace p-vdv4s
2019/12/05 13:57:07 [INFO] Creating node driver aliyunecs
2019/12/05 13:57:07 [INFO] [mgmt-cluster-rbac-delete] Updating cluster local
2019/12/05 13:57:07 [INFO] update kontainerdriver huaweicontainercloudengine
2019/12/05 13:57:07 [INFO] [mgmt-auth-prtb-controller] Creating role project-owner in namespace local
2019/12/05 13:57:07 [INFO] Creating node driver amazonec2
2019/12/05 13:57:07 [INFO] [mgmt-auth-prtb-controller] Creating roleBinding for subject user-bvqq8 with role project-owner in namespace 
2019/12/05 13:57:07 [ERROR] ProjectRoleTemplateBindingController p-vdv4s/creator-project-owner [mgmt-auth-prtb-controller] failed with : clusterroles.rbac.authorization.k8s.io "local-clustermember" already exists
2019/12/05 13:57:07 [INFO] [mgmt-auth-prtb-controller] Updating clusterRoleBinding clusterrolebinding-lqjvt for cluster membership in cluster local for subject user-bvqq8
2019/12/05 13:57:07 [INFO] Creating node driver azure
2019/12/05 13:57:07 [INFO] [mgmt-auth-prtb-controller] Creating roleBinding for subject user-bvqq8 with role project-owner in namespace 
2019/12/05 13:57:07 [INFO] [mgmt-auth-prtb-controller] Creating role project-owner in namespace p-znlft
2019/12/05 13:57:07 [INFO] Creating node driver cloudca
2019/12/05 13:57:07 [INFO] [mgmt-auth-crtb-controller] Creating roleBinding for subject user-bvqq8 with role cluster-owner in namespace 
2019/12/05 13:57:07 [INFO] [mgmt-auth-crtb-controller] Creating role cluster-owner in namespace p-znlft
2019/12/05 13:57:07 [INFO] Creating node driver digitalocean
2019/12/05 13:57:07 [INFO] [mgmt-auth-prtb-controller] Creating role project-owner in namespace p-vdv4s
2019/12/05 13:57:07 [INFO] [mgmt-auth-crtb-controller] Creating roleBinding for subject user-bvqq8 with role cluster-owner in namespace 
2019/12/05 13:57:07 [INFO] [mgmt-auth-prtb-controller] Creating roleBinding for subject user-bvqq8 with role project-owner in namespace 
2019/12/05 13:57:07 [INFO] [mgmt-auth-prtb-controller] Creating roleBinding for subject user-bvqq8 with role project-owner in namespace 
2019/12/05 13:57:07 [INFO] Creating node driver exoscale
2019/12/05 13:57:07 [INFO] Creating node driver linode
2019/12/05 13:57:07 [INFO] Creating node driver openstack
2019/12/05 13:57:07 [ERROR] NodeDriverController aliyunecs [node-driver-controller] failed with : dynamicschemas.management.cattle.io "credentialconfig" already exists
2019/12/05 13:57:07 [INFO] Creating node driver otc
2019/12/05 13:57:07 http: TLS handshake error from 127.0.0.1:54322: EOF
2019/12/05 13:57:07 http: TLS handshake error from 127.0.0.1:54324: EOF
2019/12/05 13:57:07 [INFO] Creating node driver packet
2019/12/05 13:57:07 [INFO] Stopping cluster agent for local
2019/12/05 13:57:07 [INFO] Creating node driver rackspace
2019/12/05 13:57:07 [INFO] Creating node driver softlayer
2019/12/05 13:57:07 [INFO] Creating node driver vmwarevsphere
2019/12/05 13:57:07 [INFO] Rancher startup complete
2019/12/05 13:57:07 [INFO] update kontainerdriver rancherkubernetesengine
2019/12/05 13:57:07 [INFO] uploading amazonec2Config to nodeconfig schema
2019/12/05 13:57:07 [INFO] uploading amazonec2Config to nodetemplateconfig schema
2019/12/05 13:57:07 [INFO] uploading amazonec2credentialConfig to credentialconfig schema
2019/12/05 13:57:07 [INFO] uploading digitaloceanConfig to nodeconfig schema
2019/12/05 13:57:07 [INFO] uploading azureConfig to nodeconfig schema
2019/12/05 13:57:07 [INFO] uploading azureConfig to nodetemplateconfig schema
2019/12/05 13:57:07 [INFO] uploading vmwarevsphereConfig to nodeconfig schema
2019/12/05 13:57:07 [INFO] uploading vmwarevsphereConfig to nodetemplateconfig schema
2019/12/05 13:57:07 [INFO] uploading azurecredentialConfig to credentialconfig schema
2019/12/05 13:57:07 [INFO] uploading vmwarevspherecredentialConfig to credentialconfig schema
2019/12/05 13:57:07 [INFO] update kontainerdriver baiducloudcontainerengine
W1205 13:57:07.924821    4997 shared_informer.go:312] resyncPeriod 51483487083091 is smaller than resyncCheckPeriod 70944634993586 and the informer has already started. Changing it to 70944634993586
W1205 13:57:07.925370    4997 shared_informer.go:312] resyncPeriod 51650572387629 is smaller than resyncCheckPeriod 70944634993586 and the informer has already started. Changing it to 70944634993586
E1205 13:57:07.925581    4997 resource_quota_controller.go:171] initial monitor sync has error: [couldn't start monitor for resource "management.cattle.io/v3, Resource=clustertemplaterevisions": unable to monitor quota for resource "management.cattle.io/v3, Resource=clustertemplaterevisions", couldn't start monitor for resource "management.cattle.io/v3, Resource=catalogtemplateversions": unable to monitor quota for resource "management.cattle.io/v3, Resource=catalogtemplateversions", couldn't start monitor for resource "management.cattle.io/v3, Resource=projectnetworkpolicies": unable to monitor quota for resource "management.cattle.io/v3, Resource=projectnetworkpolicies", couldn't start monitor for resource "management.cattle.io/v3, Resource=clusteralertrules": unable to monitor quota for resource "management.cattle.io/v3, Resource=clusteralertrules", couldn't start monitor for resource "management.cattle.io/v3, Resource=rkek8ssystemimages": unable to monitor quota for resource "management.cattle.io/v3, Resource=rkek8ssystemimages", couldn't start monitor for resource "management.cattle.io/v3, Resource=clusteralerts": unable to monitor quota for resource "management.cattle.io/v3, Resource=clusteralerts", couldn't start monitor for resource "management.cattle.io/v3, Resource=globaldnsproviders": unable to monitor quota for resource "management.cattle.io/v3, Resource=globaldnsproviders", couldn't start monitor for resource "project.cattle.io/v3, Resource=pipelines": unable to monitor quota for resource "project.cattle.io/v3, Resource=pipelines", couldn't start monitor for resource "project.cattle.io/v3, Resource=apps": unable to monitor quota for resource "project.cattle.io/v3, Resource=apps", couldn't start monitor for resource "management.cattle.io/v3, Resource=monitormetrics": unable to monitor quota for resource "management.cattle.io/v3, Resource=monitormetrics", couldn't start monitor for resource "management.cattle.io/v3, Resource=globaldnses": unable to monitor quota for resource "management.cattle.io/v3, Resource=globaldnses", couldn't start monitor for resource "management.cattle.io/v3, Resource=rkeaddons": unable to monitor quota for resource "management.cattle.io/v3, Resource=rkeaddons", couldn't start monitor for resource "k3s.cattle.io/v1, Resource=listenerconfigs": unable to monitor quota for resource "k3s.cattle.io/v1, Resource=listenerconfigs", couldn't start monitor for resource "management.cattle.io/v3, Resource=clusteralertgroups": unable to monitor quota for resource "management.cattle.io/v3, Resource=clusteralertgroups", couldn't start monitor for resource "management.cattle.io/v3, Resource=projects": unable to monitor quota for resource "management.cattle.io/v3, Resource=projects", couldn't start monitor for resource "project.cattle.io/v3, Resource=pipelinesettings": unable to monitor quota for resource "project.cattle.io/v3, Resource=pipelinesettings", couldn't start monitor for resource "management.cattle.io/v3, Resource=multiclusterapprevisions": unable to monitor quota for resource "management.cattle.io/v3, Resource=multiclusterapprevisions", couldn't start monitor for resource "management.cattle.io/v3, Resource=notifiers": unable to monitor quota for resource "management.cattle.io/v3, Resource=notifiers", couldn't start monitor for resource "management.cattle.io/v3, Resource=preferences": unable to monitor quota for resource "management.cattle.io/v3, Resource=preferences", couldn't start monitor for resource "management.cattle.io/v3, Resource=clusterroletemplatebindings": unable to monitor quota for resource "management.cattle.io/v3, Resource=clusterroletemplatebindings", couldn't start monitor for resource "management.cattle.io/v3, Resource=clusterregistrationtokens": unable to monitor quota for resource "management.cattle.io/v3, Resource=clusterregistrationtokens", couldn't start monitor for resource "project.cattle.io/v3, Resource=apprevisions": unable to monitor quota for resource "project.cattle.io/v3, Resource=apprevisions", couldn't start monitor for resource "management.cattle.io/v3, Resource=clustermonitorgraphs": unable to monitor quota for resource "management.cattle.io/v3, Resource=clustermonitorgraphs", couldn't start monitor for resource "management.cattle.io/v3, Resource=clusterloggings": unable to monitor quota for resource "management.cattle.io/v3, Resource=clusterloggings", couldn't start monitor for resource "management.cattle.io/v3, Resource=nodes": unable to monitor quota for resource "management.cattle.io/v3, Resource=nodes", couldn't start monitor for resource "management.cattle.io/v3, Resource=clusterscans": unable to monitor quota for resource "management.cattle.io/v3, Resource=clusterscans", couldn't start monitor for resource "project.cattle.io/v3, Resource=sourcecoderepositories": unable to monitor quota for resource "project.cattle.io/v3, Resource=sourcecoderepositories", couldn't start monitor for resource "project.cattle.io/v3, Resource=pipelineexecutions": unable to monitor quota for resource "project.cattle.io/v3, Resource=pipelineexecutions", couldn't start monitor for resource "extensions/v1beta1, Resource=networkpolicies": unable to monitor quota for resource "extensions/v1beta1, Resource=networkpolicies", couldn't start monitor for resource "management.cattle.io/v3, Resource=projectroletemplatebindings": unable to monitor quota for resource "management.cattle.io/v3, Resource=projectroletemplatebindings", couldn't start monitor for resource "management.cattle.io/v3, Resource=etcdbackups": unable to monitor quota for resource "management.cattle.io/v3, Resource=etcdbackups", couldn't start monitor for resource "management.cattle.io/v3, Resource=projectloggings": unable to monitor quota for resource "management.cattle.io/v3, Resource=projectloggings", couldn't start monitor for resource "k3s.cattle.io/v1, Resource=addons": unable to monitor quota for resource "k3s.cattle.io/v1, Resource=addons", couldn't start monitor for resource "helm.cattle.io/v1, Resource=helmcharts": unable to monitor quota for resource "helm.cattle.io/v1, Resource=helmcharts", couldn't start monitor for resource "management.cattle.io/v3, Resource=clustertemplates": unable to monitor quota for resource "management.cattle.io/v3, Resource=clustertemplates", couldn't start monitor for resource "management.cattle.io/v3, Resource=projectalertrules": unable to monitor quota for resource "management.cattle.io/v3, Resource=projectalertrules", couldn't start monitor for resource "management.cattle.io/v3, Resource=projectalertgroups": unable to monitor quota for resource "management.cattle.io/v3, Resource=projectalertgroups", couldn't start monitor for resource "project.cattle.io/v3, Resource=sourcecodecredentials": unable to monitor quota for resource "project.cattle.io/v3, Resource=sourcecodecredentials", couldn't start monitor for resource "management.cattle.io/v3, Resource=clustercatalogs": unable to monitor quota for resource "management.cattle.io/v3, Resource=clustercatalogs", couldn't start monitor for resource "management.cattle.io/v3, Resource=multiclusterapps": unable to monitor quota for resource "management.cattle.io/v3, Resource=multiclusterapps", couldn't start monitor for resource "management.cattle.io/v3, Resource=projectmonitorgraphs": unable to monitor quota for resource "management.cattle.io/v3, Resource=projectmonitorgraphs", couldn't start monitor for resource "management.cattle.io/v3, Resource=catalogtemplates": unable to monitor quota for resource "management.cattle.io/v3, Resource=catalogtemplates", couldn't start monitor for resource "management.cattle.io/v3, Resource=projectcatalogs": unable to monitor quota for resource "management.cattle.io/v3, Resource=projectcatalogs", couldn't start monitor for resource "management.cattle.io/v3, Resource=projectalerts": unable to monitor quota for resource "management.cattle.io/v3, Resource=projectalerts", couldn't start monitor for resource "management.cattle.io/v3, Resource=podsecuritypolicytemplateprojectbindings": unable to monitor quota for resource "management.cattle.io/v3, Resource=podsecuritypolicytemplateprojectbindings", couldn't start monitor for resource "management.cattle.io/v3, Resource=nodetemplates": unable to monitor quota for resource "management.cattle.io/v3, Resource=nodetemplates", couldn't start monitor for resource "management.cattle.io/v3, Resource=rkek8sserviceoptions": unable to monitor quota for resource "management.cattle.io/v3, Resource=rkek8sserviceoptions", couldn't start monitor for resource "management.cattle.io/v3, Resource=nodepools": unable to monitor quota for resource "management.cattle.io/v3, Resource=nodepools", couldn't start monitor for resource "project.cattle.io/v3, Resource=sourcecodeproviderconfigs": unable to monitor quota for resource "project.cattle.io/v3, Resource=sourcecodeproviderconfigs"]
2019/12/05 13:57:07 [INFO] update kontainerdriver aliyunkubernetescontainerservice
2019/12/05 13:57:08 [INFO] Registering project network policy
2019/12/05 13:57:08 [INFO] Registering CIS controller
2019/12/05 13:57:08 [INFO] registering podsecuritypolicy cluster handler for cluster local
2019/12/05 13:57:08 [INFO] registering podsecuritypolicy project handler for cluster local
2019/12/05 13:57:08 [INFO] registering podsecuritypolicy namespace handler for cluster local
2019/12/05 13:57:08 [INFO] registering podsecuritypolicy serviceaccount handler for cluster local
2019/12/05 13:57:08 [INFO] registering podsecuritypolicy template handler for cluster local
2019/12/05 13:57:08 [INFO] Registering monitoring for cluster "local"
2019/12/05 13:57:08 [INFO] Registering istio for cluster "local"
2019/12/05 13:57:08 [INFO] Creating CRD prometheuses.monitoring.coreos.com
2019/12/05 13:57:08 [INFO] Creating CRD prometheusrules.monitoring.coreos.com
2019/12/05 13:57:08 [INFO] Creating CRD alertmanagers.monitoring.coreos.com
2019/12/05 13:57:08 [INFO] Creating CRD servicemonitors.monitoring.coreos.com
2019/12/05 13:57:08 [INFO] Waiting for CRD prometheusrules.monitoring.coreos.com to become available
2019/12/05 13:57:08 [INFO] uploading digitaloceanConfig to nodeconfig schema
2019/12/05 13:57:08 [INFO] uploading digitaloceanConfig to nodetemplateconfig schema
2019/12/05 13:57:08 [INFO] uploading digitaloceancredentialConfig to credentialconfig schema
W1205 13:57:08.583625    4997 probe.go:268] Flexvolume plugin directory at /usr/libexec/kubernetes/kubelet-plugins/volume/exec/ does not exist. Recreating.
2019/12/05 13:57:08 [INFO] Done waiting for CRD prometheusrules.monitoring.coreos.com to become available
2019/12/05 13:57:08 [INFO] Waiting for CRD alertmanagers.monitoring.coreos.com to become available
2019/12/05 13:57:08 [INFO] Updating global catalog system-library
2019/12/05 13:57:08 [INFO] Catalog sync done. 3 templates created, 0 templates updated, 0 templates deleted
2019/12/05 13:57:09 [INFO] Done waiting for CRD alertmanagers.monitoring.coreos.com to become available
2019/12/05 13:57:09 [INFO] Waiting for CRD servicemonitors.monitoring.coreos.com to become available
2019/12/05 13:57:09 [INFO] Done waiting for CRD servicemonitors.monitoring.coreos.com to become available
2019/12/05 13:57:09 [INFO] Registering namespaceHandler for adding labels 
2019/12/05 13:57:09 [INFO] Starting cluster controllers for local
E1205 13:57:10.034101    4997 prometheus.go:138] failed to register depth metric certificate: duplicate metrics collector registration attempted
E1205 13:57:10.034135    4997 prometheus.go:150] failed to register adds metric certificate: duplicate metrics collector registration attempted
E1205 13:57:10.034168    4997 prometheus.go:162] failed to register latency metric certificate: duplicate metrics collector registration attempted
E1205 13:57:10.034219    4997 prometheus.go:174] failed to register work_duration metric certificate: duplicate metrics collector registration attempted
E1205 13:57:10.034241    4997 prometheus.go:189] failed to register unfinished_work_seconds metric certificate: duplicate metrics collector registration attempted
E1205 13:57:10.034258    4997 prometheus.go:202] failed to register longest_running_processor_microseconds metric certificate: duplicate metrics collector registration attempted
E1205 13:57:10.034288    4997 prometheus.go:214] failed to register retries metric certificate: duplicate metrics collector registration attempted
E1205 13:57:10.035317    4997 resource_quota_controller.go:437] failed to sync resource monitors: [couldn't start monitor for resource "management.cattle.io/v3, Resource=projectloggings": unable to monitor quota for resource "management.cattle.io/v3, Resource=projectloggings", couldn't start monitor for resource "management.cattle.io/v3, Resource=projectcatalogs": unable to monitor quota for resource "management.cattle.io/v3, Resource=projectcatalogs", couldn't start monitor for resource "management.cattle.io/v3, Resource=projectroletemplatebindings": unable to monitor quota for resource "management.cattle.io/v3, Resource=projectroletemplatebindings", couldn't start monitor for resource "management.cattle.io/v3, Resource=multiclusterapprevisions": unable to monitor quota for resource "management.cattle.io/v3, Resource=multiclusterapprevisions", couldn't start monitor for resource "project.cattle.io/v3, Resource=apprevisions": unable to monitor quota for resource "project.cattle.io/v3, Resource=apprevisions", couldn't start monitor for resource "management.cattle.io/v3, Resource=projectalerts": unable to monitor quota for resource "management.cattle.io/v3, Resource=projectalerts", couldn't start monitor for resource "management.cattle.io/v3, Resource=notifiers": unable to monitor quota for resource "management.cattle.io/v3, Resource=notifiers", couldn't start monitor for resource "management.cattle.io/v3, Resource=clusterscans": unable to monitor quota for resource "management.cattle.io/v3, Resource=clusterscans", couldn't start monitor for resource "management.cattle.io/v3, Resource=nodepools": unable to monitor quota for resource "management.cattle.io/v3, Resource=nodepools", couldn't start monitor for resource "extensions/v1beta1, Resource=networkpolicies": unable to monitor quota for resource "extensions/v1beta1, Resource=networkpolicies", couldn't start monitor for resource "helm.cattle.io/v1, Resource=helmcharts": unable to monitor quota for resource "helm.cattle.io/v1, Resource=helmcharts", couldn't start monitor for resource "project.cattle.io/v3, Resource=apps": unable to monitor quota for resource "project.cattle.io/v3, Resource=apps", couldn't start monitor for resource "management.cattle.io/v3, Resource=clustertemplates": unable to monitor quota for resource "management.cattle.io/v3, Resource=clustertemplates", couldn't start monitor for resource "project.cattle.io/v3, Resource=pipelineexecutions": unable to monitor quota for resource "project.cattle.io/v3, Resource=pipelineexecutions", couldn't start monitor for resource "project.cattle.io/v3, Resource=sourcecoderepositories": unable to monitor quota for resource "project.cattle.io/v3, Resource=sourcecoderepositories", couldn't start monitor for resource "management.cattle.io/v3, Resource=clusteralertgroups": unable to monitor quota for resource "management.cattle.io/v3, Resource=clusteralertgroups", couldn't start monitor for resource "management.cattle.io/v3, Resource=clustertemplaterevisions": unable to monitor quota for resource "management.cattle.io/v3, Resource=clustertemplaterevisions", couldn't start monitor for resource "management.cattle.io/v3, Resource=clusterregistrationtokens": unable to monitor quota for resource "management.cattle.io/v3, Resource=clusterregistrationtokens", couldn't start monitor for resource "management.cattle.io/v3, Resource=clustermonitorgraphs": unable to monitor quota for resource "management.cattle.io/v3, Resource=clustermonitorgraphs", couldn't start monitor for resource "management.cattle.io/v3, Resource=rkek8ssystemimages": unable to monitor quota for resource "management.cattle.io/v3, Resource=rkek8ssystemimages", couldn't start monitor for resource "management.cattle.io/v3, Resource=preferences": unable to monitor quota for resource "management.cattle.io/v3, Resource=preferences", couldn't start monitor for resource "management.cattle.io/v3, Resource=clustercatalogs": unable to monitor quota for resource "management.cattle.io/v3, Resource=clustercatalogs", couldn't start monitor for resource "management.cattle.io/v3, Resource=rkek8sserviceoptions": unable to monitor quota for resource "management.cattle.io/v3, Resource=rkek8sserviceoptions", couldn't start monitor for resource "management.cattle.io/v3, Resource=projectmonitorgraphs": unable to monitor quota for resource "management.cattle.io/v3, Resource=projectmonitorgraphs", couldn't start monitor for resource "management.cattle.io/v3, Resource=projectalertrules": unable to monitor quota for resource "management.cattle.io/v3, Resource=projectalertrules", couldn't start monitor for resource "management.cattle.io/v3, Resource=multiclusterapps": unable to monitor quota for resource "management.cattle.io/v3, Resource=multiclusterapps", couldn't start monitor for resource "management.cattle.io/v3, Resource=clusteralertrules": unable to monitor quota for resource "management.cattle.io/v3, Resource=clusteralertrules", couldn't start monitor for resource "management.cattle.io/v3, Resource=globaldnsproviders": unable to monitor quota for resource "management.cattle.io/v3, Resource=globaldnsproviders", couldn't start monitor for resource "management.cattle.io/v3, Resource=catalogtemplateversions": unable to monitor quota for resource "management.cattle.io/v3, Resource=catalogtemplateversions", couldn't start monitor for resource "k3s.cattle.io/v1, Resource=addons": unable to monitor quota for resource "k3s.cattle.io/v1, Resource=addons", couldn't start monitor for resource "management.cattle.io/v3, Resource=nodes": unable to monitor quota for resource "management.cattle.io/v3, Resource=nodes", couldn't start monitor for resource "management.cattle.io/v3, Resource=clusterroletemplatebindings": unable to monitor quota for resource "management.cattle.io/v3, Resource=clusterroletemplatebindings", couldn't start monitor for resource "k3s.cattle.io/v1, Resource=listenerconfigs": unable to monitor quota for resource "k3s.cattle.io/v1, Resource=listenerconfigs", couldn't start monitor for resource "management.cattle.io/v3, Resource=rkeaddons": unable to monitor quota for resource "management.cattle.io/v3, Resource=rkeaddons", couldn't start monitor for resource "management.cattle.io/v3, Resource=podsecuritypolicytemplateprojectbindings": unable to monitor quota for resource "management.cattle.io/v3, Resource=podsecuritypolicytemplateprojectbindings", couldn't start monitor for resource "management.cattle.io/v3, Resource=nodetemplates": unable to monitor quota for resource "management.cattle.io/v3, Resource=nodetemplates", couldn't start monitor for resource "management.cattle.io/v3, Resource=catalogtemplates": unable to monitor quota for resource "management.cattle.io/v3, Resource=catalogtemplates", couldn't start monitor for resource "management.cattle.io/v3, Resource=globaldnses": unable to monitor quota for resource "management.cattle.io/v3, Resource=globaldnses", couldn't start monitor for resource "management.cattle.io/v3, Resource=etcdbackups": unable to monitor quota for resource "management.cattle.io/v3, Resource=etcdbackups", couldn't start monitor for resource "management.cattle.io/v3, Resource=clusterloggings": unable to monitor quota for resource "management.cattle.io/v3, Resource=clusterloggings", couldn't start monitor for resource "management.cattle.io/v3, Resource=projectalertgroups": unable to monitor quota for resource "management.cattle.io/v3, Resource=projectalertgroups", couldn't start monitor for resource "project.cattle.io/v3, Resource=pipelinesettings": unable to monitor quota for resource "project.cattle.io/v3, Resource=pipelinesettings", couldn't start monitor for resource "management.cattle.io/v3, Resource=clusteralerts": unable to monitor quota for resource "management.cattle.io/v3, Resource=clusteralerts", couldn't start monitor for resource "project.cattle.io/v3, Resource=sourcecodeproviderconfigs": unable to monitor quota for resource "project.cattle.io/v3, Resource=sourcecodeproviderconfigs", couldn't start monitor for resource "management.cattle.io/v3, Resource=projectnetworkpolicies": unable to monitor quota for resource "management.cattle.io/v3, Resource=projectnetworkpolicies", couldn't start monitor for resource "management.cattle.io/v3, Resource=projects": unable to monitor quota for resource "management.cattle.io/v3, Resource=projects", couldn't start monitor for resource "management.cattle.io/v3, Resource=monitormetrics": unable to monitor quota for resource "management.cattle.io/v3, Resource=monitormetrics", couldn't start monitor for resource "project.cattle.io/v3, Resource=pipelines": unable to monitor quota for resource "project.cattle.io/v3, Resource=pipelines", couldn't start monitor for resource "project.cattle.io/v3, Resource=sourcecodecredentials": unable to monitor quota for resource "project.cattle.io/v3, Resource=sourcecodecredentials"]
2019/12/05 13:57:10 [INFO] Starting cluster agent for local [owner=true]
2019/12/05 13:57:10 [ERROR] CatalogController system-library [system-image-upgrade-catalog-controller] failed with : upgrade cluster local system service logging failed: cluster local not ready
2019/12/05 13:57:10 [INFO] Creating clusterRole for roleTemplate Project Owner (project-owner).
2019/12/05 13:57:10 [INFO] Creating clusterRole for roleTemplate Cluster Owner (cluster-owner).
2019/12/05 13:57:10 [INFO] Creating clusterRole for roleTemplate Project Owner (project-owner).
2019/12/05 13:57:10 [INFO] Updating role project-owner in p-vdv4s because of rules difference with roleTemplate Project Owner (project-owner).
2019/12/05 13:57:10 [INFO] Creating roleBinding User user-bvqq8 Role cluster-owner
2019/12/05 13:57:10 [INFO] Updating role project-owner in p-vdv4s because of rules difference with roleTemplate Project Owner (project-owner).
2019/12/05 13:57:10 [INFO] Updating role project-owner in p-znlft because of rules difference with roleTemplate Project Owner (project-owner).
2019/12/05 13:57:10 [INFO] Updating role project-owner in p-znlft because of rules difference with roleTemplate Project Owner (project-owner).
2019/12/05 13:57:10 [INFO] Creating clusterRole project-owner-promoted for project access to global resource.
2019/12/05 13:57:10 [INFO] Updating clusterRole project-owner-promoted for project access to global resource.
2019/12/05 13:57:10 [ERROR] ClusterAlertRuleController local/etcd-system-service [cluster-alert-rule-deployer] failed with : update cluster local failed, Operation cannot be fulfilled on clusters.management.cattle.io "local": the object has been modified; please apply your changes to the latest version and try again
2019/12/05 13:57:10 [WARNING] error updating ns p-vdv4s status: Operation cannot be fulfilled on namespaces "p-vdv4s": the object has been modified; please apply your changes to the latest version and try again
2019/12/05 13:57:10 [ERROR] ClusterAlertRuleController local/deployment-event-alert [cluster-alert-rule-deployer] failed with : update cluster local failed, Operation cannot be fulfilled on clusters.management.cattle.io "local": the object has been modified; please apply your changes to the latest version and try again
2019/12/05 13:57:10 [ERROR] ClusterAlertRuleController local/no-leader [cluster-alert-rule-deployer] failed with : update cluster local failed, Operation cannot be fulfilled on clusters.management.cattle.io "local": the object has been modified; please apply your changes to the latest version and try again
2019/12/05 13:57:10 [INFO] Updating clusterRole project-owner-promoted for project access to global resource.
2019/12/05 13:57:10 http: TLS handshake error from 127.0.0.1:55294: EOF
2019/12/05 13:57:10 [INFO] Creating roleBinding User user-bvqq8 Role admin
2019/12/05 13:57:10 [ERROR] ClusterAlertRuleController local/node-disk-running-full [cluster-alert-rule-deployer] failed with : update cluster local failed, Operation cannot be fulfilled on clusters.management.cattle.io "local": the object has been modified; please apply your changes to the latest version and try again
2019/12/05 13:57:10 [INFO] Creating roleBinding User user-bvqq8 Role project-owner
2019/12/05 13:57:10 [ERROR] ClusterAlertGroupController local/kube-components-alert [cluster-alert-group-deployer] failed with : update cluster local failed, Operation cannot be fulfilled on clusters.management.cattle.io "local": the object has been modified; please apply your changes to the latest version and try again
2019/12/05 13:57:11 [ERROR] ClusterAlertRuleController local/high-cpu-load [cluster-alert-rule-deployer] failed with : update cluster local failed, Operation cannot be fulfilled on clusters.management.cattle.io "local": the object has been modified; please apply your changes to the latest version and try again
2019/12/05 13:57:11 [INFO] Updating clusterRole project-owner-promoted for project access to global resource.
2019/12/05 13:57:11 [INFO] Updating global catalog library
2019/12/05 13:57:11 [INFO] Updating global catalog system-library
2019/12/05 13:57:11 [INFO] Creating clusterRole for roleTemplate Create Namespaces (create-ns).
2019/12/05 13:57:11 [INFO] Creating clusterRoleBinding for project access to global resource for subject user-bvqq8 role p-vdv4s-namespaces-edit.
W1205 13:57:11.125406    4997 reflector.go:289] k8s.io/kubernetes/pkg/controller/garbagecollector/graph_builder.go:124: watch of <nil> ended with: too old resource version: 1045 (1049)
2019/12/05 13:57:11 [ERROR] ClusterAlertGroupController local/node-alert [cluster-alert-group-deployer] failed with : update cluster local failed, Operation cannot be fulfilled on clusters.management.cattle.io "local": the object has been modified; please apply your changes to the latest version and try again
2019/12/05 13:57:11 [ERROR] ClusterAlertGroupController local/event-alert [cluster-alert-group-deployer] failed with : update cluster local failed, Operation cannot be fulfilled on clusters.management.cattle.io "local": the object has been modified; please apply your changes to the latest version and try again
2019/12/05 13:57:11 [INFO] Creating roleBinding User user-bvqq8 Role project-owner
2019/12/05 13:57:11 [WARNING] error updating ns local status: Operation cannot be fulfilled on namespaces "local": the object has been modified; please apply your changes to the latest version and try again
2019/12/05 13:57:11 [INFO] Creating roleBinding User user-bvqq8 Role project-owner
2019/12/05 13:57:11 [WARNING] error updating ns p-znlft status: Operation cannot be fulfilled on namespaces "p-znlft": the object has been modified; please apply your changes to the latest version and try again
2019/12/05 13:57:11 [INFO] Creating clusterRoleBinding for project access to global resource for subject user-bvqq8 role project-owner-promoted.
2019/12/05 13:57:11 [ERROR] CatalogController system-library [system-image-upgrade-catalog-controller] failed with : upgrade cluster local system service logging failed: cluster local not ready
2019/12/05 13:57:11 [INFO] Creating roleBinding User user-bvqq8 Role project-owner
2019/12/05 13:57:11 [INFO] Creating roleBinding User user-bvqq8 Role admin
2019/12/05 13:57:11 [INFO] Creating roleBinding User user-bvqq8 Role admin
2019/12/05 13:57:11 [INFO] Creating roleBinding User user-bvqq8 Role project-owner
2019/12/05 13:57:11 [INFO] Creating clusterRoleBinding for project access to global resource for subject user-bvqq8 role project-owner-promoted.
2019/12/05 13:57:11 [INFO] Creating roleBinding User user-bvqq8 Role admin
2019/12/05 13:57:11 [INFO] Creating roleBinding User user-bvqq8 Role admin
2019/12/05 13:57:11 [INFO] Creating roleBinding User user-bvqq8 Role admin
2019/12/05 13:57:11 [INFO] Creating clusterRoleBinding for project access to global resource for subject user-bvqq8 role create-ns.
2019/12/05 13:57:11 [INFO] [mgmt-auth-prtb-controller] Creating role admin in namespace p-vdv4s
2019/12/05 13:57:11 [INFO] [mgmt-auth-prtb-controller] Creating role admin in namespace p-znlft
2019/12/05 13:57:11 [INFO] namespaceHandler: addProjectIDLabelToNamespace: adding label field.cattle.io/projectId=p-vdv4s to namespace=default
2019/12/05 13:57:11 [INFO] Creating roleBinding User user-bvqq8 Role project-owner
2019/12/05 13:57:11 [INFO] Creating roleBinding User user-bvqq8 Role admin
2019/12/05 13:57:11 [INFO] [mgmt-auth-prtb-controller] Creating roleBinding for subject user-bvqq8 with role admin in namespace 
2019/12/05 13:57:11 [INFO] [mgmt-auth-prtb-controller] Creating roleBinding for subject user-bvqq8 with role admin in namespace 
2019/12/05 13:57:11 [INFO] Creating roleBinding User user-bvqq8 Role project-owner
2019/12/05 13:57:11 [INFO] Creating roleBinding User user-bvqq8 Role project-owner
2019/12/05 13:57:11 [INFO] [mgmt-auth-prtb-controller] Creating roleBinding for subject user-bvqq8 with role admin in namespace 
2019/12/05 13:57:11 [INFO] Creating clusterRoleBinding for project access to global resource for subject user-bvqq8 role p-znlft-namespaces-edit.
2019/12/05 13:57:11 [INFO] Creating roleBinding User user-bvqq8 Role admin
2019/12/05 13:57:11 [INFO] Updating clusterRoleBinding clusterrolebinding-pkn2p for project access to global resource for subject user-bvqq8 role project-owner-promoted.
2019/12/05 13:57:11 [INFO] Updating clusterRoleBinding clusterrolebinding-q49g9 for project access to global resource for subject user-bvqq8 role project-owner-promoted.
2019/12/05 13:57:11 [INFO] Updating clusterRoleBinding clusterrolebinding-q49g9 for project access to global resource for subject user-bvqq8 role project-owner-promoted.
2019/12/05 13:57:11 [INFO] namespaceHandler: addProjectIDLabelToNamespace: adding label field.cattle.io/projectId=p-znlft to namespace=kube-node-lease
2019/12/05 13:57:11 [ERROR] CatalogController system-library [system-image-upgrade-catalog-controller] failed with : upgrade cluster local system service logging failed: cluster local not ready
2019/12/05 13:57:11 [INFO] namespaceHandler: addProjectIDLabelToNamespace: adding label field.cattle.io/projectId=p-znlft to namespace=kube-public
2019/12/05 13:57:11 [INFO] namespaceHandler: addProjectIDLabelToNamespace: adding label field.cattle.io/projectId=p-znlft to namespace=cattle-system
2019/12/05 13:57:11 [INFO] Creating user for principal system://p-vdv4s
2019/12/05 13:57:11 [INFO] Creating user for principal system://p-znlft
2019/12/05 13:57:11 [INFO] Deleting roleBinding clusterrolebinding-55djd
2019/12/05 13:57:11 [INFO] Creating globalRoleBindings for u-4pmqin7gb3
2019/12/05 13:57:11 [INFO] Creating globalRoleBindings for u-dumkgudang
2019/12/05 13:57:11 [INFO] namespaceHandler: addProjectIDLabelToNamespace: adding label field.cattle.io/projectId=p-znlft to namespace=cattle-global-data
2019/12/05 13:57:11 [INFO] Creating new GlobalRoleBinding for GlobalRoleBinding grb-gttkh
2019/12/05 13:57:11 [INFO] [mgmt-auth-grb-controller] Creating clusterRoleBinding for globalRoleBinding grb-gttkh for user u-dumkgudang with role cattle-globalrole-user
2019/12/05 13:57:11 [INFO] Deleting roleBinding clusterrolebinding-bd66b
2019/12/05 13:57:11 [INFO] Deleting roleBinding clusterrolebinding-pndjg
2019/12/05 13:57:11 [ERROR] CatalogController system-library [system-image-upgrade-catalog-controller] failed with : upgrade cluster local system service logging failed: cluster local not ready
2019/12/05 13:57:11 [INFO] Deleting roleBinding clusterrolebinding-dgvdh
2019/12/05 13:57:11 [INFO] Creating new GlobalRoleBinding for GlobalRoleBinding grb-5xbhx
2019/12/05 13:57:11 [INFO] [mgmt-auth-grb-controller] Creating clusterRoleBinding for globalRoleBinding grb-5xbhx for user u-4pmqin7gb3 with role cattle-globalrole-user
2019/12/05 13:57:11 [INFO] namespaceHandler: addProjectIDLabelToNamespace: adding label field.cattle.io/projectId=p-znlft to namespace=cattle-system
2019/12/05 13:57:12 [ERROR] namespaceHandler: Sync: error adding project id label to namespace err=Operation cannot be fulfilled on namespaces "cattle-system": the object has been modified; please apply your changes to the latest version and try again
2019/12/05 13:57:12 [ERROR] CatalogController system-library [system-image-upgrade-catalog-controller] failed with : upgrade cluster local system service logging failed: cluster local not ready
2019/12/05 13:57:12 [INFO] Creating clusterRole for roleTemplate Project Member (project-member).
2019/12/05 13:57:12 [ERROR] CatalogController system-library [system-image-upgrade-catalog-controller] failed with : upgrade cluster local system service logging failed: cluster local not ready
2019/12/05 13:57:12 [INFO] [mgmt-auth-prtb-controller] Creating clusterRole p-znlft-projectmember
2019/12/05 13:57:12 [INFO] [mgmt-auth-prtb-controller] Creating clusterRole p-vdv4s-projectmember
2019/12/05 13:57:12 [INFO] [mgmt-auth-prtb-controller] Creating roleBinding for membership in project p-znlft for subject u-4pmqin7gb3
2019/12/05 13:57:12 [ERROR] ProjectController local/p-znlft [system-image-upgrade-controller] failed with : upgrade cluster local system service logging failed: cluster local not ready
2019/12/05 13:57:12 [INFO] Creating roleBinding User u-dumkgudang Role project-member
2019/12/05 13:57:12 [INFO] Creating roleBinding User u-4pmqin7gb3 Role edit
2019/12/05 13:57:12 [INFO] [mgmt-auth-prtb-controller] Creating clusterRoleBinding for membership in cluster local for subject u-4pmqin7gb3
2019/12/05 13:57:12 [INFO] Creating roleBinding User u-dumkgudang Role project-member
2019/12/05 13:57:12 [INFO] Creating roleBinding User u-dumkgudang Role edit
2019/12/05 13:57:12 [INFO] Creating roleBinding User u-4pmqin7gb3 Role project-member
2019/12/05 13:57:12 [INFO] [mgmt-auth-prtb-controller] Creating roleBinding for membership in project p-vdv4s for subject u-dumkgudang
2019/12/05 13:57:12 [INFO] Creating roleBinding User u-dumkgudang Role edit
2019/12/05 13:57:12 [INFO] [mgmt-auth-prtb-controller] Creating role project-member in namespace local
2019/12/05 13:57:12 [ERROR] ProjectController local/p-vdv4s [system-image-upgrade-controller] failed with : upgrade cluster local system service logging failed: cluster local not ready
2019/12/05 13:57:12 [INFO] [mgmt-auth-prtb-controller] Creating roleBinding for subject u-4pmqin7gb3 with role project-member in namespace 
2019/12/05 13:57:12 [INFO] Creating clusterRole project-member-promoted for project access to global resource.
2019/12/05 13:57:12 [INFO] Creating roleBinding User u-4pmqin7gb3 Role project-member
2019/12/05 13:57:12 [INFO] [mgmt-auth-prtb-controller] Creating clusterRoleBinding for membership in cluster local for subject u-dumkgudang
2019/12/05 13:57:12 [INFO] Updating clusterRole project-member-promoted for project access to global resource.
2019/12/05 13:57:12 [INFO] [mgmt-auth-prtb-controller] Creating roleBinding for subject u-dumkgudang with role project-member in namespace 
2019/12/05 13:57:12 [INFO] [mgmt-auth-prtb-controller] Creating role project-member in namespace p-znlft
2019/12/05 13:57:12 [INFO] Creating roleBinding User u-4pmqin7gb3 Role edit
2019/12/05 13:57:12 [ERROR] CatalogController system-library [system-image-upgrade-catalog-controller] failed with : upgrade cluster local system service logging failed: cluster local not ready
2019/12/05 13:57:12 [INFO] [mgmt-auth-prtb-controller] Creating role project-member in namespace p-vdv4s
2019/12/05 13:57:12 [INFO] namespaceHandler: addProjectIDLabelToNamespace: adding label field.cattle.io/projectId=p-znlft to namespace=kube-system
2019/12/05 13:57:12 [INFO] Updating clusterRole project-member-promoted for project access to global resource.
2019/12/05 13:57:12 [INFO] Creating roleBinding User u-4pmqin7gb3 Role edit
2019/12/05 13:57:12 [INFO] [mgmt-auth-prtb-controller] Creating role edit in namespace p-znlft
2019/12/05 13:57:12 [INFO] Updating role project-member in p-vdv4s because of rules difference with roleTemplate Project Member (project-member).
2019/12/05 13:57:12 [INFO] Updating role project-member in p-vdv4s because of rules difference with roleTemplate Project Member (project-member).
2019/12/05 13:57:12 [INFO] Creating roleBinding User u-4pmqin7gb3 Role project-member
2019/12/05 13:57:12 [INFO] [mgmt-auth-prtb-controller] Creating role edit in namespace p-vdv4s
2019/12/05 13:57:12 [INFO] [mgmt-auth-prtb-controller] Creating roleBinding for subject u-4pmqin7gb3 with role project-member in namespace 
2019/12/05 13:57:12 [INFO] Creating roleBinding User u-4pmqin7gb3 Role project-member
2019/12/05 13:57:12 [INFO] Updating role project-member in p-znlft because of rules difference with roleTemplate Project Member (project-member).
2019/12/05 13:57:12 [INFO] kontainerdriver googlekubernetesengine listening on address 127.0.0.1:34825
2019/12/05 13:57:12 [INFO] kontainerdriver googlekubernetesengine stopped
2019/12/05 13:57:12 [INFO] [mgmt-auth-prtb-controller] Creating roleBinding for subject u-dumkgudang with role project-member in namespace 
2019/12/05 13:57:12 [INFO] Creating roleBinding User u-4pmqin7gb3 Role edit
2019/12/05 13:57:12 [INFO] [mgmt-auth-prtb-controller] Creating roleBinding for subject u-dumkgudang with role edit in namespace 
2019/12/05 13:57:12 [INFO] [mgmt-auth-prtb-controller] Creating roleBinding for subject u-4pmqin7gb3 with role edit in namespace 
2019/12/05 13:57:12 [ERROR] CatalogController system-library [system-image-upgrade-catalog-controller] failed with : upgrade cluster local system service logging failed: cluster local not ready
2019/12/05 13:57:12 [INFO] update kontainerdriver googlekubernetesengine
2019/12/05 13:57:12 [INFO] Creating roleBinding User u-4pmqin7gb3 Role project-member
2019/12/05 13:57:12 [INFO] kontainerdriver azurekubernetesservice listening on address 127.0.0.1:39228
2019/12/05 13:57:12 [INFO] kontainerdriver azurekubernetesservice stopped
2019/12/05 13:57:12 [INFO] Creating roleBinding User u-4pmqin7gb3 Role edit
2019/12/05 13:57:12 [INFO] Catalog sync done. 2 templates created, 3 templates updated, 0 templates deleted
2019/12/05 13:57:12 [INFO] Updating clusterRole project-member-promoted for project access to global resource.
2019/12/05 13:57:12 [INFO] kontainerdriver amazonelasticcontainerservice listening on address 127.0.0.1:38856
2019/12/05 13:57:12 [INFO] kontainerdriver amazonelasticcontainerservice stopped
2019/12/05 13:57:12 [INFO] Creating clusterRoleBinding for project access to global resource for subject u-4pmqin7gb3 role p-znlft-namespaces-edit.
2019/12/05 13:57:12 [ERROR] CatalogController system-library [system-image-upgrade-catalog-controller] failed with : upgrade cluster local system service logging failed: cluster local not ready
2019/12/05 13:57:12 [INFO] update kontainerdriver azurekubernetesservice
2019/12/05 13:57:12 [INFO] Deleting roleBinding clusterrolebinding-4szcc
2019/12/05 13:57:12 [INFO] Creating clusterRoleBinding for project access to global resource for subject u-4pmqin7gb3 role project-member-promoted.
2019/12/05 13:57:12 [INFO] Creating clusterRoleBinding for project access to global resource for subject u-4pmqin7gb3 role create-ns.
2019/12/05 13:57:12 [INFO] update kontainerdriver amazonelasticcontainerservice
2019/12/05 13:57:12 [INFO] Deleting roleBinding clusterrolebinding-x4z8x
2019/12/05 13:57:12 [INFO] Creating clusterRoleBinding for project access to global resource for subject u-dumkgudang role p-vdv4s-namespaces-edit.
2019/12/05 13:57:12 [INFO] Creating clusterRoleBinding for project access to global resource for subject u-dumkgudang role project-member-promoted.
2019/12/05 13:57:12 [INFO] Creating clusterRoleBinding for project access to global resource for subject u-dumkgudang role create-ns.
2019/12/05 13:57:12 [INFO] namespaceHandler: addProjectIDLabelToNamespace: adding label field.cattle.io/projectId=p-znlft to namespace=kube-system
2019/12/05 13:57:12 [ERROR] namespaceHandler: Sync: error adding project id label to namespace err=Operation cannot be fulfilled on namespaces "kube-system": the object has been modified; please apply your changes to the latest version and try again
2019/12/05 13:57:12 [ERROR] ProjectController local/p-znlft [system-image-upgrade-controller] failed with : upgrade cluster local system service logging failed: cluster local not ready
2019/12/05 13:57:12 [ERROR] ProjectController local/p-vdv4s [system-image-upgrade-controller] failed with : upgrade cluster local system service logging failed: cluster local not ready
2019/12/05 13:57:12 [ERROR] ProjectController local/p-znlft [system-image-upgrade-controller] failed with : upgrade cluster local system service logging failed: cluster local not ready
2019/12/05 13:57:12 [ERROR] ProjectController local/p-vdv4s [system-image-upgrade-controller] failed with : upgrade cluster local system service logging failed: cluster local not ready
2019/12/05 13:57:12 [ERROR] ProjectController local/p-znlft [system-image-upgrade-controller] failed with : upgrade cluster local system service logging failed: cluster local not ready
2019/12/05 13:57:12 [ERROR] ProjectController local/p-vdv4s [system-image-upgrade-controller] failed with : upgrade cluster local system service logging failed: cluster local not ready
2019/12/05 13:57:13 [ERROR] ProjectController local/p-znlft [system-image-upgrade-controller] failed with : upgrade cluster local system service logging failed: cluster local not ready
2019/12/05 13:57:13 [ERROR] ProjectController local/p-vdv4s [system-image-upgrade-controller] failed with : upgrade cluster local system service logging failed: cluster local not ready
2019/12/05 13:57:13 [ERROR] ProjectController local/p-vdv4s [system-image-upgrade-controller] failed with : upgrade cluster local system service logging failed: cluster local not ready
2019/12/05 13:57:13 [INFO] driverMetadata: refresh data
2019/12/05 13:57:13 [ERROR] CatalogController system-library [system-image-upgrade-catalog-controller] failed with : upgrade cluster local system service logging failed: cluster local not ready
2019/12/05 13:57:13 [INFO] Updating clusterRoleBinding clusterrolebinding-8q9qh for project access to global resource for subject user-bvqq8 role create-ns.
2019/12/05 13:57:13 [INFO] Catalog sync done. 42 templates created, 0 templates updated, 0 templates deleted
2019/12/05 13:57:14 [ERROR] CatalogController library [catalog] failed with : failed to sync templates. Resetting commit. Multiple error occurred: [yaml: line 4: did not find expected key]
2019/12/05 13:57:14 [INFO] driverMetadata initialized successfully
2019/12/05 13:57:15 [INFO] Updating global catalog library
2019/12/05 13:57:15 [INFO] Catalog sync done. 0 templates created, 0 templates updated, 0 templates deleted
2019/12/05 13:57:16 [ERROR] ProjectController local/p-znlft [system-image-upgrade-controller] failed with : upgrade cluster local system service logging failed: cluster local not ready
2019/12/05 13:57:16 [ERROR] ProjectController local/p-vdv4s [system-image-upgrade-controller] failed with : upgrade cluster local system service logging failed: cluster local not ready
2019/12/05 13:57:17 [INFO] kontainerdriver googlekubernetesengine listening on address 127.0.0.1:44372
2019/12/05 13:57:17 [INFO] kontainerdriver googlekubernetesengine stopped
2019/12/05 13:57:17 [INFO] dynamic schema for kontainerdriver googlekubernetesengine updating
2019/12/05 13:57:17 [INFO] kontainerdriver azurekubernetesservice listening on address 127.0.0.1:41066
2019/12/05 13:57:17 [INFO] kontainerdriver azurekubernetesservice stopped
2019/12/05 13:57:17 [INFO] dynamic schema for kontainerdriver azurekubernetesservice updating
2019/12/05 13:57:17 [INFO] kontainerdriver amazonelasticcontainerservice listening on address 127.0.0.1:33088
2019/12/05 13:57:17 [INFO] kontainerdriver amazonelasticcontainerservice stopped
2019/12/05 13:57:17 [INFO] dynamic schema for kontainerdriver amazonelasticcontainerservice updating
E1205 13:57:40.387270    4997 resource_quota_controller.go:437] failed to sync resource monitors: [couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=alertmanagers": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=alertmanagers", couldn't start monitor for resource "management.cattle.io/v3, Resource=preferences": unable to monitor quota for resource "management.cattle.io/v3, Resource=preferences", couldn't start monitor for resource "management.cattle.io/v3, Resource=projects": unable to monitor quota for resource "management.cattle.io/v3, Resource=projects", couldn't start monitor for resource "project.cattle.io/v3, Resource=sourcecodecredentials": unable to monitor quota for resource "project.cattle.io/v3, Resource=sourcecodecredentials", couldn't start monitor for resource "management.cattle.io/v3, Resource=monitormetrics": unable to monitor quota for resource "management.cattle.io/v3, Resource=monitormetrics", couldn't start monitor for resource "management.cattle.io/v3, Resource=clustertemplates": unable to monitor quota for resource "management.cattle.io/v3, Resource=clustertemplates", couldn't start monitor for resource "project.cattle.io/v3, Resource=sourcecodeproviderconfigs": unable to monitor quota for resource "project.cattle.io/v3, Resource=sourcecodeproviderconfigs", couldn't start monitor for resource "management.cattle.io/v3, Resource=clusterscans": unable to monitor quota for resource "management.cattle.io/v3, Resource=clusterscans", couldn't start monitor for resource "management.cattle.io/v3, Resource=projectcatalogs": unable to monitor quota for resource "management.cattle.io/v3, Resource=projectcatalogs", couldn't start monitor for resource "management.cattle.io/v3, Resource=clusteralertgroups": unable to monitor quota for resource "management.cattle.io/v3, Resource=clusteralertgroups", couldn't start monitor for resource "management.cattle.io/v3, Resource=multiclusterapps": unable to monitor quota for resource "management.cattle.io/v3, Resource=multiclusterapps", couldn't start monitor for resource "management.cattle.io/v3, Resource=projectmonitorgraphs": unable to monitor quota for resource "management.cattle.io/v3, Resource=projectmonitorgraphs", couldn't start monitor for resource "project.cattle.io/v3, Resource=apps": unable to monitor quota for resource "project.cattle.io/v3, Resource=apps", couldn't start monitor for resource "project.cattle.io/v3, Resource=pipelineexecutions": unable to monitor quota for resource "project.cattle.io/v3, Resource=pipelineexecutions", couldn't start monitor for resource "k3s.cattle.io/v1, Resource=listenerconfigs": unable to monitor quota for resource "k3s.cattle.io/v1, Resource=listenerconfigs", couldn't start monitor for resource "management.cattle.io/v3, Resource=catalogtemplateversions": unable to monitor quota for resource "management.cattle.io/v3, Resource=catalogtemplateversions", couldn't start monitor for resource "management.cattle.io/v3, Resource=clustermonitorgraphs": unable to monitor quota for resource "management.cattle.io/v3, Resource=clustermonitorgraphs", couldn't start monitor for resource "management.cattle.io/v3, Resource=globaldnses": unable to monitor quota for resource "management.cattle.io/v3, Resource=globaldnses", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=prometheusrules": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=prometheusrules", couldn't start monitor for resource "management.cattle.io/v3, Resource=nodetemplates": unable to monitor quota for resource "management.cattle.io/v3, Resource=nodetemplates", couldn't start monitor for resource "project.cattle.io/v3, Resource=apprevisions": unable to monitor quota for resource "project.cattle.io/v3, Resource=apprevisions", couldn't start monitor for resource "management.cattle.io/v3, Resource=clusterloggings": unable to monitor quota for resource "management.cattle.io/v3, Resource=clusterloggings", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=servicemonitors": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=servicemonitors", couldn't start monitor for resource "management.cattle.io/v3, Resource=catalogtemplates": unable to monitor quota for resource "management.cattle.io/v3, Resource=catalogtemplates", couldn't start monitor for resource "management.cattle.io/v3, Resource=podsecuritypolicytemplateprojectbindings": unable to monitor quota for resource "management.cattle.io/v3, Resource=podsecuritypolicytemplateprojectbindings", couldn't start monitor for resource "management.cattle.io/v3, Resource=clusteralerts": unable to monitor quota for resource "management.cattle.io/v3, Resource=clusteralerts", couldn't start monitor for resource "project.cattle.io/v3, Resource=pipelinesettings": unable to monitor quota for resource "project.cattle.io/v3, Resource=pipelinesettings", couldn't start monitor for resource "extensions/v1beta1, Resource=networkpolicies": unable to monitor quota for resource "extensions/v1beta1, Resource=networkpolicies", couldn't start monitor for resource "management.cattle.io/v3, Resource=clusterregistrationtokens": unable to monitor quota for resource "management.cattle.io/v3, Resource=clusterregistrationtokens", couldn't start monitor for resource "management.cattle.io/v3, Resource=projectroletemplatebindings": unable to monitor quota for resource "management.cattle.io/v3, Resource=projectroletemplatebindings", couldn't start monitor for resource "project.cattle.io/v3, Resource=sourcecoderepositories": unable to monitor quota for resource "project.cattle.io/v3, Resource=sourcecoderepositories", couldn't start monitor for resource "management.cattle.io/v3, Resource=clustercatalogs": unable to monitor quota for resource "management.cattle.io/v3, Resource=clustercatalogs", couldn't start monitor for resource "management.cattle.io/v3, Resource=projectalertrules": unable to monitor quota for resource "management.cattle.io/v3, Resource=projectalertrules", couldn't start monitor for resource "management.cattle.io/v3, Resource=clustertemplaterevisions": unable to monitor quota for resource "management.cattle.io/v3, Resource=clustertemplaterevisions", couldn't start monitor for resource "management.cattle.io/v3, Resource=clusterroletemplatebindings": unable to monitor quota for resource "management.cattle.io/v3, Resource=clusterroletemplatebindings", couldn't start monitor for resource "management.cattle.io/v3, Resource=projectnetworkpolicies": unable to monitor quota for resource "management.cattle.io/v3, Resource=projectnetworkpolicies", couldn't start monitor for resource "management.cattle.io/v3, Resource=rkek8sserviceoptions": unable to monitor quota for resource "management.cattle.io/v3, Resource=rkek8sserviceoptions", couldn't start monitor for resource "k3s.cattle.io/v1, Resource=addons": unable to monitor quota for resource "k3s.cattle.io/v1, Resource=addons", couldn't start monitor for resource "monitoring.coreos.com/v1, Resource=prometheuses": unable to monitor quota for resource "monitoring.coreos.com/v1, Resource=prometheuses", couldn't start monitor for resource "management.cattle.io/v3, Resource=clusteralertrules": unable to monitor quota for resource "management.cattle.io/v3, Resource=clusteralertrules", couldn't start monitor for resource "management.cattle.io/v3, Resource=projectalertgroups": unable to monitor quota for resource "management.cattle.io/v3, Resource=projectalertgroups", couldn't start monitor for resource "project.cattle.io/v3, Resource=pipelines": unable to monitor quota for resource "project.cattle.io/v3, Resource=pipelines", couldn't start monitor for resource "management.cattle.io/v3, Resource=rkeaddons": unable to monitor quota for resource "management.cattle.io/v3, Resource=rkeaddons", couldn't start monitor for resource "management.cattle.io/v3, Resource=projectloggings": unable to monitor quota for resource "management.cattle.io/v3, Resource=projectloggings", couldn't start monitor for resource "management.cattle.io/v3, Resource=etcdbackups": unable to monitor quota for resource "management.cattle.io/v3, Resource=etcdbackups", couldn't start monitor for resource "management.cattle.io/v3, Resource=multiclusterapprevisions": unable to monitor quota for resource "management.cattle.io/v3, Resource=multiclusterapprevisions", couldn't start monitor for resource "management.cattle.io/v3, Resource=globaldnsproviders": unable to monitor quota for resource "management.cattle.io/v3, Resource=globaldnsproviders", couldn't start monitor for resource "management.cattle.io/v3, Resource=projectalerts": unable to monitor quota for resource "management.cattle.io/v3, Resource=projectalerts", couldn't start monitor for resource "management.cattle.io/v3, Resource=notifiers": unable to monitor quota for resource "management.cattle.io/v3, Resource=notifiers", couldn't start monitor for resource "management.cattle.io/v3, Resource=nodes": unable to monitor quota for resource "management.cattle.io/v3, Resource=nodes", couldn't start monitor for resource "management.cattle.io/v3, Resource=rkek8ssystemimages": unable to monitor quota for resource "management.cattle.io/v3, Resource=rkek8ssystemimages", couldn't start monitor for resource "helm.cattle.io/v1, Resource=helmcharts": unable to monitor quota for resource "helm.cattle.io/v1, Resource=helmcharts", couldn't start monitor for resource "management.cattle.io/v3, Resource=nodepools": unable to monitor quota for resource "management.cattle.io/v3, Resource=nodepools"]
2019/12/05 13:57:46 [INFO] Creating token for user user-bvqq8
2019/12/05 13:57:46 [INFO] Deleting nodePool [np-brb4h]
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:57:46 [ERROR] logan to nil: <nil>
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:57:46 [ERROR] logan to nil: <nil>
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:57:46 [ERROR] logan to nil: <nil>
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:57:46 [ERROR] logan to nil: <nil>
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:57:46 [ERROR] logan to nil: <nil>
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:57:46 [ERROR] logan to nil: <nil>
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:57:46 [ERROR] logan to nil: <nil>
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:57:46 [ERROR] logan to nil: <nil>
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:57:46 [ERROR] logan to nil: <nil>
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:57:46 [ERROR] logan to nil: <nil>
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:57:46 [ERROR] logan to nil: <nil>
2019/12/05 13:57:46 [INFO] Provisioning node test1
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:57:46 [ERROR] logan to nil: <nil>
2019/12/05 13:57:46 [INFO] [node-controller-docker-machine] Creating CA: management-state/node/nodes/test1/certs/ca.pem
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:57:46 [ERROR] logan to nil: <nil>
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:57:46 [ERROR] logan to nil: <nil>
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:57:46 [ERROR] logan to nil: <nil>
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:57:46 [ERROR] logan to nil: <nil>
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:57:46 [ERROR] logan to nil: <nil>
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:57:46 [ERROR] logan to nil: <nil>
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:57:46 [ERROR] logan to nil: <nil>
2019/12/05 13:57:46 [INFO] Provisioning node test2
2019/12/05 13:57:46 [INFO] [node-controller-docker-machine] Creating CA: management-state/node/nodes/test2/certs/ca.pem
2019/12/05 13:57:46 [INFO] Deleting nodePool [np-xxbcc]
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:57:46 [ERROR] logan to nil: <nil>
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:57:46 [ERROR] logan to nil: <nil>
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:57:46 [ERROR] logan to nil: <nil>
2019/12/05 13:57:46 [INFO] [node-controller-docker-machine] Creating client certificate: management-state/node/nodes/test2/certs/cert.pem
2019/12/05 13:57:46 [INFO] Creating user for principal system://local
2019/12/05 13:57:46 [INFO] Creating globalRoleBindings for u-b4qkhsnliz
2019/12/05 13:57:46 [INFO] Creating new GlobalRoleBinding for GlobalRoleBinding grb-tbbcm
2019/12/05 13:57:46 [INFO] [mgmt-auth-grb-controller] Creating clusterRoleBinding for globalRoleBinding grb-tbbcm for user u-b4qkhsnliz with role cattle-globalrole-user
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] Redeploy Rancher Agents is needed: forceDeploy=false, agent/auth image changed=true, private repo changed=false
2019/12/05 13:57:46 [INFO] Creating token for user u-b4qkhsnliz
2019/12/05 13:57:46 [INFO] [mgmt-auth-crtb-controller] Creating clusterRoleBinding for membership in cluster local for subject u-b4qkhsnliz
2019/12/05 13:57:46 [INFO] [mgmt-auth-crtb-controller] Creating roleBinding for subject u-b4qkhsnliz with role cluster-owner in namespace 
2019/12/05 13:57:46 [INFO] Creating roleBinding User u-b4qkhsnliz Role cluster-owner
2019/12/05 13:57:46 [ERROR] ClusterController local [cluster-deploy] failed with : waiting for server-url setting to be set
2019/12/05 13:57:46 [INFO] Redeploy Rancher Agents is needed: forceDeploy=false, agent/auth image changed=true, private repo changed=false
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:57:46 [ERROR] logan to nil: <nil>
2019/12/05 13:57:46 [INFO] [mgmt-auth-crtb-controller] Creating roleBinding for subject u-b4qkhsnliz with role cluster-owner in namespace 
2019/12/05 13:57:46 [ERROR] ClusterController local [cluster-deploy] failed with : waiting for server-url setting to be set
2019/12/05 13:57:46 [INFO] [mgmt-auth-crtb-controller] Creating roleBinding for subject u-b4qkhsnliz with role cluster-owner in namespace 
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:57:46 [ERROR] logan to nil: <nil>
2019/12/05 13:57:46 [INFO] Deleting nodePool [np-nvps2]
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:57:46 [ERROR] logan to nil: <nil>
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:57:46 [ERROR] logan to nil: <nil>
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:57:46 [ERROR] logan to nil: <nil>
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:57:46 [ERROR] logan to nil: <nil>
2019/12/05 13:57:46 [INFO] [node-controller-docker-machine] Running pre-create checks...
2019/12/05 13:57:46 [INFO] [node-controller-docker-machine] Creating client certificate: management-state/node/nodes/test1/certs/cert.pem
2019/12/05 13:57:46 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:46 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:57:46 [ERROR] logan to nil: <nil>
2019/12/05 13:57:47 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:47 [INFO] [node-controller-docker-machine] Running pre-create checks...
2019/12/05 13:57:47 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:47 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:57:47 [ERROR] logan to nil: <nil>
2019/12/05 13:57:48 [INFO] Creating new GlobalRoleBinding for GlobalRoleBinding grb-9kwjb
2019/12/05 13:57:48 [INFO] [mgmt-auth-grb-controller] Creating clusterRoleBinding for globalRoleBinding grb-9kwjb for user u-vl54b with role cattle-globalrole-user
2019/12/05 13:57:48 [INFO] Creating new GlobalRoleBinding for GlobalRoleBinding grb-9dbbx
2019/12/05 13:57:48 [INFO] [mgmt-auth-grb-controller] Creating clusterRoleBinding for globalRoleBinding grb-9dbbx for user u-z76h2 with role cattle-globalrole-user
2019/12/05 13:57:49 [INFO] Generating and uploading node config 
2019/12/05 13:57:49 [INFO] Creating new GlobalRoleBinding for GlobalRoleBinding grb-k9vkf
2019/12/05 13:57:49 [INFO] [mgmt-auth-grb-controller] Creating clusterRoleBinding for globalRoleBinding grb-k9vkf for user u-sjwl5 with role cattle-globalrole-user
2019/12/05 13:57:49 [INFO] [mgmt-auth-users-controller] Deleting token token-xlmr6 for user u-z76h2
2019/12/05 13:57:49 [INFO] Generating and uploading node config test1
2019/12/05 13:57:49 [ERROR] GlobalRoleBindingController grb-k9vkf [mgmt-auth-grb-controller] failed with : globalrolebindings.management.cattle.io "grb-k9vkf" not found
2019/12/05 13:57:49 [INFO] [mgmt-auth-users-controller] Deleting token token-bpm78 for user u-sjwl5
2019/12/05 13:57:50 [ERROR] GlobalRoleBindingController grb-9kwjb [grb-sync] failed with : globalrolebindings.management.cattle.io "grb-9kwjb" not found
2019/12/05 13:57:50 [INFO] [mgmt-auth-users-controller] Deleting token token-78wrj for user u-vl54b
2019/12/05 13:57:50 [INFO] Creating new GlobalRoleBinding for GlobalRoleBinding grb-rmjgq
2019/12/05 13:57:50 [INFO] [mgmt-auth-grb-controller] Creating clusterRoleBinding for globalRoleBinding grb-rmjgq for user u-5dhgk with role cattle-globalrole-user
2019/12/05 13:57:50 [INFO] Creating new GlobalRoleBinding for GlobalRoleBinding grb-h8st2
2019/12/05 13:57:50 [INFO] [mgmt-auth-grb-controller] Creating clusterRoleBinding for globalRoleBinding grb-h8st2 for user u-cfxfq with role cattle-globalrole-user
2019/12/05 13:57:50 [INFO] [mgmt-auth-users-controller] Deleting globalRoleBinding grb-rmjgq for user u-5dhgk
2019/12/05 13:57:50 [INFO] [mgmt-auth-users-controller] Deleting token token-mng8l for user u-5dhgk
2019/12/05 13:57:50 [ERROR] GlobalRoleBindingController grb-rmjgq [mgmt-auth-grb-controller] failed with : globalrolebindings.management.cattle.io "grb-rmjgq" not found
2019/12/05 13:57:50 [INFO] ========== start syncLabels   ==========
2019/12/05 13:57:50 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:57:50 [ERROR] logan to nil: <nil>
2019/12/05 13:57:50 [INFO] Deleting nodePool [np-4pstz]
2019/12/05 13:57:50 [INFO] [mgmt-auth-users-controller] Deleting globalRoleBinding grb-h8st2 for user u-cfxfq
2019/12/05 13:57:50 [ERROR] UserController u-cfxfq [mgmt-auth-users-controller] failed with : error deleting global role template grb-h8st2: globalrolebindings.management.cattle.io "grb-h8st2" not found
2019/12/05 13:57:50 [ERROR] GlobalRoleBindingController grb-h8st2 [mgmt-auth-grb-controller] failed with : globalrolebindings.management.cattle.io "grb-h8st2" not found
2019/12/05 13:57:50 [INFO] [mgmt-auth-users-controller] Deleting token token-7wfsc for user u-cfxfq
2019/12/05 13:58:04 [ERROR] NodeController local/m-9swrg [node-controller] failed with : Error with pre-create check: "ssh-key-fingerprint needs to be provided for \"/tmp/S4MDLNL3SR/id_rsa\""
2019/12/05 13:58:04 [INFO] ========== start syncLabels   ==========
2019/12/05 13:58:04 [INFO] ========== jiandao start syncLabels  1 ==========
2019/12/05 13:58:04 [ERROR] logan to nil: <nil>
2019/12/05 13:58:04 [INFO] ========== start syncLabels   ==========
2019/12/05 13:58:04 [ERROR] NodeController local/m-9swrg [node-controller] failed with : nodes.management.cattle.io "m-9swrg" not found
2019/12/05 13:58:04 [ERROR] NodeController local/m-lp6px [node-controller] failed with : Error with pre-create check: "Failed to authenticate using service principal credentials: Unexpected response from Get Subscription: subscriptions.Client#Get: Failure responding to request: StatusCode=404 -- Original Error: autorest/azure: Service returned an error. Status=404 Code=\"SubscriptionNotFound\" Message=\"The subscription 'test' could not be found.\""
2019/12/05 13:58:04 [INFO] Provisioning node test1
2019/12/05 13:58:04 [INFO] [node-controller-docker-machine] Running pre-create checks...
2019/12/05 13:58:07 [INFO] Generating and uploading node config test1
2019/12/05 13:58:10 [INFO] Shutting down TokenController controller
2019/12/05 13:58:10 [FATAL] context canceled
***END RANCHER LOGS***
Makefile:11: recipe for target 'ci' failed
